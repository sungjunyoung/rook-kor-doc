<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Rook (Ceph) Korean Documentation</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="indroduction.html">Introduction</a></li><li class="chapter-item expanded "><a href="rook.html"><strong aria-hidden="true">1.</strong> Rook</a></li><li class="chapter-item expanded "><a href="quickstart.html"><strong aria-hidden="true">2.</strong> Quickstart</a></li><li class="chapter-item expanded "><a href="prerequisites/prerequisites.html"><strong aria-hidden="true">3.</strong> Prerequisites</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="prerequisites/authenticated_registries.html"><strong aria-hidden="true">3.1.</strong> Authenticated Registries</a></li><li class="chapter-item expanded "><a href="prerequisites/pod_security_policies.html"><strong aria-hidden="true">3.2.</strong> Pod Security Policies</a></li></ol></li><li class="chapter-item expanded "><a href="ceph_storage/ceph_storage.html"><strong aria-hidden="true">4.</strong> Ceph Storage</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ceph_storage/admission_controller.html"><strong aria-hidden="true">4.1.</strong> Admission Controller</a></li><li class="chapter-item expanded "><a href="ceph_storage/examples.html"><strong aria-hidden="true">4.2.</strong> Examples</a></li><li class="chapter-item expanded "><a href="ceph_storage/openshift.html"><strong aria-hidden="true">4.3.</strong> Openshift</a></li><li class="chapter-item expanded "><a href="ceph_storage/block_storage.html"><strong aria-hidden="true">4.4.</strong> Block Storage</a></li><li class="chapter-item expanded "><a href="ceph_storage/object_storage.html"><strong aria-hidden="true">4.5.</strong> Object Storage</a></li><li class="chapter-item expanded "><a href="ceph_storage/shared_filesystem.html"><strong aria-hidden="true">4.6.</strong> Shared Filesystem</a></li><li class="chapter-item expanded "><a href="ceph_storage/ceph_dashboard.html"><strong aria-hidden="true">4.7.</strong> Ceph Dashboard</a></li><li class="chapter-item expanded "><a href="ceph_storage/prometheus_monitoring.html"><strong aria-hidden="true">4.8.</strong> Prometheus Monitoring</a></li><li class="chapter-item expanded "><a href="ceph_storage/cluster_crd.html"><strong aria-hidden="true">4.9.</strong> Cluster CRD</a></li><li class="chapter-item expanded "><a href="ceph_storage/block_pool_crd.html"><strong aria-hidden="true">4.10.</strong> Block Pool CRD</a></li><li class="chapter-item expanded "><a href="ceph_storage/object_store_crd.html"><strong aria-hidden="true">4.11.</strong> Object Store CRD</a></li><li class="chapter-item expanded "><a href="ceph_storage/object_multisite_crds.html"><strong aria-hidden="true">4.12.</strong> Object Multisite CRDs</a></li><li class="chapter-item expanded "><a href="ceph_storage/object_bucket_claim.html"><strong aria-hidden="true">4.13.</strong> Object Bucket Claim</a></li><li class="chapter-item expanded "><a href="ceph_storage/object_store_user_crd.html"><strong aria-hidden="true">4.14.</strong> Object Store User CRD</a></li><li class="chapter-item expanded "><a href="ceph_storage/bucket_notifications.html"><strong aria-hidden="true">4.15.</strong> Bucket Notifications</a></li><li class="chapter-item expanded "><a href="ceph_storage/shared_filesstem_crd.html"><strong aria-hidden="true">4.16.</strong> Shared Filesystem CRD</a></li><li class="chapter-item expanded "><a href="ceph_storage/nfs_crd.html"><strong aria-hidden="true">4.17.</strong> NFS CRD</a></li><li class="chapter-item expanded "><a href="ceph_storage/ceph_csi.html"><strong aria-hidden="true">4.18.</strong> Ceph CSI</a></li><li class="chapter-item expanded "><a href="ceph_storage/rbd_mirroring.html"><strong aria-hidden="true">4.19.</strong> RBD Mirroring</a></li><li class="chapter-item expanded "><a href="ceph_storage/failover_and_failback.html"><strong aria-hidden="true">4.20.</strong> Failover and Failback</a></li><li class="chapter-item expanded "><a href="ceph_storage/snapshots.html"><strong aria-hidden="true">4.21.</strong> Snapshots</a></li><li class="chapter-item expanded "><a href="ceph_storage/volume_clone.html"><strong aria-hidden="true">4.22.</strong> Volume clone</a></li><li class="chapter-item expanded "><a href="ceph_storage/client_crd.html"><strong aria-hidden="true">4.23.</strong> Client CRD</a></li><li class="chapter-item expanded "><a href="ceph_storage/rbdmirror_crd.html"><strong aria-hidden="true">4.24.</strong> RBDMirror CRD</a></li><li class="chapter-item expanded "><a href="ceph_storage/filesystemmirror_crd.html"><strong aria-hidden="true">4.25.</strong> FilesystemMirror CRD</a></li><li class="chapter-item expanded "><a href="ceph_storage/subvolumegroup_crd.html"><strong aria-hidden="true">4.26.</strong> SubVolumeGroup CRD</a></li><li class="chapter-item expanded "><a href="ceph_storage/key_management_system.html"><strong aria-hidden="true">4.27.</strong> Key Management System</a></li><li class="chapter-item expanded "><a href="ceph_storage/configuration.html"><strong aria-hidden="true">4.28.</strong> Configuration</a></li><li class="chapter-item expanded "><a href="ceph_storage/upgrades.html"><strong aria-hidden="true">4.29.</strong> Upgrades</a></li><li class="chapter-item expanded "><a href="ceph_storage/cleanup.html"><strong aria-hidden="true">4.30.</strong> Cleanup</a></li></ol></li><li class="chapter-item expanded "><a href="helm_charts/helm_charts.html"><strong aria-hidden="true">5.</strong> Helm Charts</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="helm_charts/ceph_operator.html"><strong aria-hidden="true">5.1.</strong> Ceph Operator</a></li><li class="chapter-item expanded "><a href="helm_charts/ceph_cluster.html"><strong aria-hidden="true">5.2.</strong> Ceph Cluster</a></li></ol></li><li class="chapter-item expanded "><a href="common_issues.html"><strong aria-hidden="true">6.</strong> Common Issues</a></li><li class="chapter-item expanded "><a href="ceph_tools/ceph_tools.html"><strong aria-hidden="true">7.</strong> Ceph Tools</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ceph_tools/toolbox.html"><strong aria-hidden="true">7.1.</strong> Toolbox</a></li><li class="chapter-item expanded "><a href="ceph_tools/common_issues.html"><strong aria-hidden="true">7.2.</strong> Common Issues</a></li><li class="chapter-item expanded "><a href="ceph_tools/csi_common_issues.html"><strong aria-hidden="true">7.3.</strong> CSI Common Issues</a></li><li class="chapter-item expanded "><a href="ceph_tools/monitor_health.html"><strong aria-hidden="true">7.4.</strong> Monitor Health</a></li><li class="chapter-item expanded "><a href="ceph_tools/osd_management.html"><strong aria-hidden="true">7.5.</strong> OSD Management</a></li><li class="chapter-item expanded "><a href="ceph_tools/direct_tools.html"><strong aria-hidden="true">7.6.</strong> Direct Tools</a></li><li class="chapter-item expanded "><a href="ceph_tools/advanced_configuration.html"><strong aria-hidden="true">7.7.</strong> Advanced Configuration</a></li><li class="chapter-item expanded "><a href="ceph_tools/openshift_common_issues.html"><strong aria-hidden="true">7.8.</strong> Openshift Common Issues</a></li><li class="chapter-item expanded "><a href="ceph_tools/disaster_recovery.html"><strong aria-hidden="true">7.9.</strong> Disaster Recovery</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Rook (Ceph) Korean Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>개인 공부를 위한 목적으로 작성된 문서입니다. 잘못된 번역이 있을 수 있으며 Openshift 및 일부 작성자에게 불필요한 문서는 번역되어 있지 않습니다.</p>
<blockquote>
<p>PR 은 언제든 환영합니다 :)</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rook"><a class="header" href="#rook">Rook</a></h1>
<p>Rook 은 플랫폼, 프레임워크를 제공하고, cloud-native 환경과 통합된 다양한 스토리지를 지원하는 오픈소스 <strong>cloud native 스토리지 오케스트레이터</strong> 입니다.</p>
<p>Rook 은 배포, 부트스트래핑, 설정, 프로비저닝, 스케일링, 업그레이드, 마이그레이션, 재난 복구, 모니터링, 리소스 관리를 자동화하여 스토리지 소프트웨어를 self-managing, self-scailing, self-healing 스토리지 서비스로 바꿔줍니다. Rook 은 cloud-native 컨테이너 관리, 스케줄링, 오케스트레이션 플랫폼 아래에서 그 역할을 수행합니다.</p>
<p>Rook 은 (Kubernetes 의) 확장 기능을 사용하여 cloud native 환경에 통합됩니다. 그리고, 스케줄링, 라이프사이클 관리, 리소스 관리, 보안, 모니터링, 그리고 유저 경험에 있어 seamless 한 경험을 제공합니.</p>
<p>Ceph operator 는 프로덕션 스토리지 플랫폼을 몇년간 제공하며 2018년 12월에 Rook v0.9 가 릴리즈되며 안정화되었습니다.</p>
<h2 id="quick-start-guide"><a class="header" href="#quick-start-guide">Quick Start Guide</a></h2>
<p>몇 가지의 <code>kubectl</code> 커맨드를 사용하여 Ceph 을 클러스터에 구성할 수 있습니다. <a href="./quickstart.html">QuickStart</a> 가이드를 참고하여 Ceph operator 를 시작하세요!</p>
<h2 id="designs"><a class="header" href="#designs">Designs</a></h2>
<p><a href="https://docs.ceph.com/en/latest/">Ceph</a> 은 블록 스토리지, 오브젝트 스토리지, 공유 파일시스템에 프로덕션 레벨로 수년간 안정화된 확장성있는 분산 스토리지 솔루션입니다. <a href="https://rook.io/docs/rook/v1.8/ceph-storage.html">Ceph overview</a> 를 참고하세요.</p>
<p>자세한 디자인 문서는, <a href="https://github.com/rook/rook/tree/master/design">design docs</a> 를 참고하세요.</p>
<h2 id="need-help-be-sure-to-join-the-rook-slack"><a class="header" href="#need-help-be-sure-to-join-the-rook-slack">Need help? Be sure to join the Rook Slack</a></h2>
<p>질문이 있다면, 주저하지 말고 <a href="https://rook-io.slack.com/">Slack channel</a> 에 문의하세요. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quickstart"><a class="header" href="#quickstart">Quickstart</a></h1>
<p>Rook 에 오신 것을 환영합니다! 여러분의 쿠버네티스 클러스터에 확장 가능하고 견고한 Ceph 스토리지를 <strong>cloud-native 스토리지 오케스트레이터 플랫폼</strong> 을 사용하여 설치하는 최상의 경험을 제공하길 희망합니다.</p>
<p>질문이 있다면 주저하지 말고 <a href="https://rook-io.slack.com/">Slack channel</a> 에 문의하세요. </p>
<p>이 가이드는 Ceph 클러스터의 기본 셋업 및 여러분의 클러스터에서 Pod 들이 블록, 오브젝트, 파일 스토리지를 사용할 수 있도록 도와줄 것입니다.</p>
<p><strong>Rook 를 테스트 하기 위해서는 항상 VM 을 사용하세요. 호스트 시스템의 로컬 디바이스가 실수로 사용되지 않도록 해야 합니다.</strong></p>
<h2 id="minimum-version"><a class="header" href="#minimum-version">Minimum Version</a></h2>
<p>Rook 지원을 위해서 쿠버네티스 v1.16 혹은 그 이상이 필요합니다.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p><a href="./prerequisites.html">Prerequisites</a> 에 따라 쿠버네티스 클러스터가 <code>Rook</code> 을 설치하기 위해 준비가 되었는지 확인해야 합니다.</p>
<p>Ceph 스토리지 클러스터를 설정하기 위해서, 아래 중 적어도 하나의 스토리지 옵션이 필요합니다:</p>
<ul>
<li>Raw 디바이스 (파티션이 없고, 포맷되어있지 않아야함)
<ul>
<li>이 요구사항은 호스트에 <code>lvm2</code> 가 설치되어 있어야 합니다. 이 디펜던시를 피하기 위해서는, 디스크에 single full-disk 파티션 (아래 참고) 을 만들어야 합니다.</li>
</ul>
</li>
<li>Raw 파티션 (포맷되어 있지 않아야 함)</li>
<li>StorageClass 로 PersistentVolume 이 <code>block</code> 모드로 생성 가능해야함</li>
</ul>
<h2 id="tldr"><a class="header" href="#tldr">TL;DR</a></h2>
<p>간단한 Rook 클러스터는 아래와 같이 kubectl 커맨드와 <a href="https://github.com/rook/rook/blob/release-1.8/deploy/examples">example manifests</a> 로 만들 수 있습니다.</p>
<pre><code class="language-sh">$ git clone --single-branch --branch v1.8.5 https://github.com/rook/rook.git
cd rook/deploy/examples
kubectl create -f crds.yaml -f common.yaml -f operator.yaml
kubectl create -f cluster.yaml
</code></pre>
<p>클러스터가 Running 상태가 되면, <a href="quickstart.html#storage">블록, 오브젝트 혹은 파일</a> 스토리지를 여러분의 어플리케이션에서 사용할 수 있습니다.</p>
<h2 id="deploy-the-rook-operator"><a class="header" href="#deploy-the-rook-operator">Deploy the Rook Operator</a></h2>
<p>첫번째 스텝은 Rook operator 를 배포하는 것입니다. 원하는 Rook 릴리즈 버전의 <a href="https://github.com/rook/rook/blob/release-1.8/deploy/examples">example yaml files</a> 을 사용하는지 확인해야 합니다. 더 많은 옵션은, <a href="/ceph_storage/examples.html">examples 문서</a> 를 확인하세요.</p>
<pre><code class="language-sh">cd deploy/examples
kubectl create -f crds.yaml -f common.yaml -f operator.yaml

# verify the rook-ceph-operator is in the `Running` state before proceeding
kubectl -n rook-ceph get pod
</code></pre>
<p><a href="/helm_charts/ceph_operator.html">Rook Helm Chart</a> 로 배포할 수도 있습니다.</p>
<p>프로덕션에 배포하기 전에 고려해야 할 사항이 몇가지 존재합니다.</p>
<ol>
<li>디폴트로 비활성화 되어 있는 Rook 기능들을 활성화하기를 원하다면, 추가 설정을 위해 <a href="https://github.com/rook/rook/blob/release-1.8/deploy/examples/operator.yaml">operator.yaml</a> 을 확인하세요.
<ol>
<li>Device discovery: <code>ROOK_ENABLE_DISCOVERY_DAEMON</code> 설정이 활성화 되어 있다면, 일반적으로 베어메탈 클러스터에서 사용되어 신규 디바이스를 감지하고 설정합니다.</li>
<li>Node affinity and tolerations: CSI 드라이버는 기본적으로 어떤 노드에나 스케줄링 될 수 있습니다. CSI 드라이버 affinity 를 지정하기 위한 여러 설정들이 존재합니다.</li>
</ol>
</li>
</ol>
<p><code>rook-ceph</code> 네임스페이스 외에 다른 네임스페이스에 설치하기 원하다면, <a href="/ceph_tools/advanced_configuration.html#using-alternate-namespaces">Ceph advanced configuration 섹션</a> 을 참고하세요.</p>
<h2 id="cluster-environments"><a class="header" href="#cluster-environments">Cluster Environments</a></h2>
<p>Rook 문서는 프로덕션 환경에서 Rook 을 시작하는 것에 초점을 맞추고 있습니다. 
물론 테스트 환겨을 위한 예제들도 제공하고 있으며, 가이드 외에 다른 환경에 클러스터를 구축하길 원한다면, 아래 예제 클러스터 manifest 들을 참고하세요.</p>
<ul>
<li><a href="https://github.com/rook/rook/blob/release-1.8/deploy/examples/cluster.yaml">cluster.yaml</a>: 베어메탈, 프로덕션 클러스터에서 설정. 적어도 세개의 워커 노드가 있어야 합니다.</li>
<li><a href="https://github.com/rook/rook/blob/release-1.8/deploy/examples/cluster-on-pvc.yaml">cluster-on-pvc.yaml</a>: 여러 클라우드 환경에서 프로덕션 클러스터를 설치하기 위한 설정</li>
<li><a href="https://github.com/rook/rook/blob/release-1.8/deploy/examples/cluster-test.yaml">cluster-test.yaml</a>: minikube 와 같은 테스트 환경에서 설치하기 위한 설정</li>
</ul>
<p>더 자세한 정보는 <a href="/ceph_storage/examples.html">Ceph examples</a> 를 참고하세요.</p>
<h2 id="create-a-ceph-cluster"><a class="header" href="#create-a-ceph-cluster">Create a Ceph Cluster</a></h2>
<p>이제 Rook operator 가 클러스터에 설치되고, Ceph 클러스터를 만들 수 있게 되었습니다. 클러스터가 리부팅으로부터 온전하기 위해서, <code>dataDirHostPath</code> 프로퍼티가 호스트에서 잘 설정되어 있는지 확인하세요. 더 많은 설정은, <a href="/ceph_storage/cluster_crd.html">configuring the cluster</a> 를 참고하세요.</p>
<p>Ceph 클러스터를 생성합니다.</p>
<pre><code class="language-sh">kubectl create -f cluster.yaml
</code></pre>
<p><code>rook-ceph</code> 네임스페이스에 Pod 리스트를 확인하기 위해서 <code>kubectl</code> 커맨드를 사용하세요. 모든 Pod 들이 Running 상태라면 아래와 같은 결과를 확인할 수 있습니다. osd pod 들의 갯수는 클러스터 내의 노드와 디바이스 갯수에 따라 다릅니다. <code>cluster.yaml</code> 파일을 수정하지 않았다면, OSD 는 노드의 갯수만큼 만들어집니다. </p>
<blockquote>
<p>만약 <code>rook-ceph-mon</code>, <code>rook-ceph-mgr</code>, <code>rook-ceph-osd</code> pod 이 생성되지 않았다면, <a href="/ceph_tools/common_issues.html">Ceph common issues</a> 을 확인하세요.</p>
</blockquote>
<pre><code class="language-sh">kubectl -n rook-ceph get pod
</code></pre>
<pre><code class="language-sh">NAME                                                 READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-provisioner-d77bb49c6-n5tgs         5/5     Running     0          140s
csi-cephfsplugin-provisioner-d77bb49c6-v9rvn         5/5     Running     0          140s
csi-cephfsplugin-rthrp                               3/3     Running     0          140s
csi-rbdplugin-hbsm7                                  3/3     Running     0          140s
csi-rbdplugin-provisioner-5b5cd64fd-nvk6c            6/6     Running     0          140s
csi-rbdplugin-provisioner-5b5cd64fd-q7bxl            6/6     Running     0          140s
rook-ceph-crashcollector-minikube-5b57b7c5d4-hfldl   1/1     Running     0          105s
rook-ceph-mgr-a-64cd7cdf54-j8b5p                     1/1     Running     0          77s
rook-ceph-mon-a-694bb7987d-fp9w7                     1/1     Running     0          105s
rook-ceph-mon-b-856fdd5cb9-5h2qk                     1/1     Running     0          94s
rook-ceph-mon-c-57545897fc-j576h                     1/1     Running     0          85s
rook-ceph-operator-85f5b946bd-s8grz                  1/1     Running     0          92m
rook-ceph-osd-0-6bb747b6c5-lnvb6                     1/1     Running     0          23s
rook-ceph-osd-1-7f67f9646d-44p7v                     1/1     Running     0          24s
rook-ceph-osd-2-6cd4b776ff-v4d68                     1/1     Running     0          25s
rook-ceph-osd-prepare-node1-vx2rz                    0/2     Completed   0          60s
rook-ceph-osd-prepare-node2-ab3fd                    0/2     Completed   0          60s
rook-ceph-osd-prepare-node3-w4xyz                    0/2     Completed   0          60s
</code></pre>
<p>클러스터가 healty 상태임이 확인되면, <a href="/ceph_tools/toolbox.html">Rook tollbox</a> 에 연결하여 <code>ceph status</code> 커맨드를 입력합니다.</p>
<ul>
<li>모든 mon 들이 quorum 내에 존재함</li>
<li>mgr 이 active 상태</li>
<li>하나 이상의 OSD 가 active 상태</li>
<li>health 가 <code>HEALTH_OK</code> 상태가 아니라면, warning 혹은 error 가 나타납니다.</li>
</ul>
<pre><code class="language-sh">ceph status
</code></pre>
<pre><code class="language-sh"> cluster:
   id:     a0452c76-30d9-4c1a-a948-5d8405f19a7c
   health: HEALTH_OK

 services:
   mon: 3 daemons, quorum a,b,c (age 3m)
   mgr: a(active, since 2m)
   osd: 3 osds: 3 up (since 1m), 3 in (since 1m)
...
</code></pre>
<p>클러스터가 healty 상태가 아니라면, <a href="/ceph_tools/common_issues.html">Ceph common issues</a> 를 참고하세요.</p>
<h2 id="storage"><a class="header" href="#storage">Storage</a></h2>
<p>Rook 에 의해서 세 타입의 스토리지를 사용할 수 있습니다. 각각의 가이드 링크를 참고하세요.</p>
<ul>
<li><a href="/ceph_storage/block_storage.html">Block</a>: Pod 에 의해 사용되는 블록 스토리지를 생성합니다. (RWO)</li>
<li><a href="/ceph_storage/shared_filesystem.html">Shared Filesystem</a>: 여러 Pod 에 걸쳐 공유되는 파일시스템을 생성합니다. (RWX)</li>
<li><a href="/ceph_storage/object_storage.html">Object</a>: 쿠버네티스 클러스터 안밖에서 사용가능한 오브젝트 스토어를 만듭니다.</li>
</ul>
<h2 id="ceph-dashboard"><a class="header" href="#ceph-dashboard">Ceph Dashboard</a></h2>
<p>Ceph 은 클러스터 상태를 확인할 수 있는 대시보드를 제공합니다. <a href="/ceph_storage/ceph_dashboard.html">dashboard guide</a> 를 참고하세요.</p>
<h2 id="tools"><a class="header" href="#tools">Tools</a></h2>
<p>toolbox pod 은 Rook 클러스터를 디버깅하고 트러블슈팅 할 수 있는 ceph admin client 권한을 가집니다. 셋업하고 사용하기 위해서 <a href="/ceph_tools/toolbox.html">toolbox documentation</a> 를 참고하세요. 유지보수와 튜닝을 위해서는 <a href="/ceph_tools/advanced_configuration.html">advanced configuration</a> 을 참고하세요.</p>
<h2 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h2>
<p>Rook 클러스터는 <a href="https://prometheus.io/">Prometheus</a> 를 통해 매트릭을 제공합니다. Rook 클러스터에 모니터링을 세팅하기 위해서는, <a href="/ceph_storage/prometheus_monitoring.html">monitoring guide</a> 를 참고하세요.</p>
<h2 id="teardown"><a class="header" href="#teardown">Teardown</a></h2>
<p>클러스터 테스트가 완료되었다면, 클러스터 클린업을 위해서 <a href="/ceph_storage/cleanup.html">다음의</a> 가이드를 참고하세요.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prerequisite"><a class="header" href="#prerequisite">Prerequisite</a></h1>
<p>Rook 는 최소 버전 조건을 충족하고, 필요한 권한만 부여되어 있다면 어떤 쿠버네티스 클러스터에서도 설치가 가능합니다. (자세한 정보는 아래를 참고하세요.)</p>
<h2 id="minimum-version-1"><a class="header" href="#minimum-version-1">Minimum Version</a></h2>
<p>Ceph operator 를 위해 Kubernetes v1.16 버전 이상이 필요합니다.</p>
<h2 id="ceph-prerequistes"><a class="header" href="#ceph-prerequistes">Ceph Prerequistes</a></h2>
<p>Ceph 스토리지 클러스터를 설정하기 위해, 아래 로컬 스토리지 옵션 중 하나를 충족해야 합니다:</p>
<ul>
<li>Raw 디바이스 (파티션이 없고, 포맷되어있지 않아야함)</li>
<li>Raw 파티션 (포맷되어 있지 않아야 함)</li>
<li>StorageClass 로 PersistentVolume 이 <code>block</code> 모드로 생성 가능해야 함</li>
</ul>
<p>아래 커맨드로 파티션이나 디바이스가 파일시스템으로 포맷되었는지 확인할 수 있습니다.</p>
<pre><code class="language-sh">lsblk -f
</code></pre>
<pre><code class="language-sh">NAME                  FSTYPE      LABEL UUID                                   MOUNTPOINT
vda
└─vda1                LVM2_member       &gt;eSO50t-GkUV-YKTH-WsGq-hNJY-eKNf-3i07IB
 ├─ubuntu--vg-root   ext4              c2366f76-6e21-4f10-a8f3-6776212e2fe4   /
 └─ubuntu--vg-swap_1 swap              9492a3dc-ad75-47cd-9596-678e8cf17ff9   [SWAP]
vdb
</code></pre>
<p><code>FSTYPE</code> 필드가 비어있지 않다면, 해당 디바이스에 파일시스템이 존재하는 것입니다. 위 예제에서는 <code>vdb</code> 를 사용할 수 있지만, <code>vda</code> 는 사용할 수 없습니다.</p>
<h2 id="lvm-package"><a class="header" href="#lvm-package">LVM package</a></h2>
<p>Ceph OSD 는 아래 시나리오에서 LVM 에 디펜던시를 가지고 있습니다.</p>
<ul>
<li>OSD 가 raw 디바이스나 파티션에 생성될 때</li>
<li>암호화 옵션이 켜져 있을 때 (cluster 커스텀 리소스의 <code>encryptedDevice: true</code>)</li>
<li><code>metadata</code> 디바이스가 지정되어 있을 때</li>
</ul>
<p>다음과 같은 상황에서는 LVM 이 필요하지 않습니다.</p>
<ul>
<li><code>storageClassDeviceSets</code> 를 사용해 PVC 위에 OSD 를 생성할 떄</li>
</ul>
<p>여러분의 시나리오에서 LVM 이 필요하다면, OSD 가 실행될 호스트에서 LVM 이 설치되어 있어야 합니다. 
몇몇 리눅스 배포판에서는 <code>lvm2</code> 패키지가 설치되어 있지 않습니다. LVM 패키지 없이는, 노드가 재시작 될 때 OSD Pod 이 정상적으로 시작되지 않습니다. 
리눅스 배포판의 패키지 매니저로 LVM 패키지 설치가 권장됩니다.</p>
<p>CentOS:</p>
<pre><code class="language-sh">sudo yum install -y lvm2
</code></pre>
<p>Ubuntu:</p>
<pre><code class="language-sh">sudo apt-get install -y lvm2
</code></pre>
<p>RancherOS:</p>
<ul>
<li><a href="https://github.com/rancher/os/issues/2551">1.5.0</a> 부터 LVM 이 지원됩니다.</li>
<li>논리적인 볼륨은 부팅 프로세스에서는 <a href="https://github.com/rook/rook/issues/5027">활성화되지 않습니다.</a>. 가능하게 하기 위해서는 <a href="https://rancher.com/docs/os/v1.x/en/installation/configuration/running-commands/">runcmd command</a> 를 추가해야 합니다.</li>
</ul>
<pre><code class="language-yaml">runcmd:
- [ vgchange, -ay ]
</code></pre>
<h2 id="kernel"><a class="header" href="#kernel">Kernel</a></h2>
<h3 id="rbd"><a class="header" href="#rbd">RBD</a></h3>
<p>Ceph 은 RBD 모듈이 포함된 리눅스 커널이 필요합니다. 대부분의 리눅스 배포판에는 이 모듈이 포함되어 있지만, 아닌 배포판도 존재합니다. (GKE Container-Optimised OS)</p>
<p>쿠버네티스 노드에서 <code>modeprobe rbd</code> 커맨드로 확인이 가능합니다. 만약 'not found' 라고 나온다면, 다른 리눅스 배포판으로 커널을 다시 빌드해야 합니다.</p>
<h3 id="cephfs"><a class="header" href="#cephfs">Cephfs</a></h3>
<p>Ceph shared filesystem (Cephfs) 볼륨을 만들고자 한다면, 4.17 이상의 커널 버전이 권장됩니다. 커널 버전이 4.17 미만이라면, 요청된 PVC 사이즈 쿼타가 정상적으로 제한되지 않습니다.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="authenticated-registries"><a class="header" href="#authenticated-registries">Authenticated Registries</a></h1>
<blockquote>
<p><a href="https://rook.io/docs/rook/v1.8/authenticated-registry.html">생략</a></p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pod-security-policies"><a class="header" href="#pod-security-policies">Pod Security Policies</a></h1>
<blockquote>
<p><a href="https://rook.io/docs/rook/v1.8/pod-security-policies.html">생략</a></p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ceph-storage"><a class="header" href="#ceph-storage">Ceph Storage</a></h1>
<p>Ceph 은 <strong>블록 스토리지</strong>, <strong>오브젝트 스토리지</strong>, 그리고 <strong>공유 파일시스템</strong> 을 지원하는 수년간 프로덕션 환경에서 사용된 확장성있는 분산 스토리지 솔루션입니다.</p>
<h2 id="design"><a class="header" href="#design">Design</a></h2>
<p>Rook 은 쿠버네티스의 요소들을 사용해서 Ceph 스토리지를 쿠버네티스 클러스터에서 동작하게 만들어 줍니다. 쿠버네티스 위에서 Ceph 이 동작함과 동시에, 쿠버네티스의 어플리케이션들은 Rook 에 의해 관리되는 블록 디바이스나 파일시스템을 마운트하여 사용할 수 있습니다. 또한, S3/Swift API 를 통해 오브젝트 스토리지를 사용할수도 있습니다. Rook 오퍼레이터는 스토리지가 정상적인 상태로 유지되도록 모니터링하고 스토리지 컴포넌트들을 설정합니다.</p>
<p>Rok 오퍼레이터는 스토리지 클러스터를 부트스트랩하고 모니터링 할 수 있는 모든 요소들을 갖춘 단순한 컨테이너입니다. 이 오퍼레이터는 <a href="ceph_storage//ceph_tools/monitor_health.html">Ceph monitor pod</a>과 OSD 데몬, 다른 Ceph 데몬들을 시작하고 모니터링합니다. 또한 서비스를 실행시키는데 필요한 리소스들과 Pod 을 초기화하고 Pool, 오브젝트 스토어, 파일시스템을 위한 CRD 를 관리합니다.</p>
<p>오퍼레이터는 클러스터가 정상적인 상태가 되도록 모니터링 합니다. Ceph mon 들이 오퍼레이터에 의해 시작되거나, 필요에 따라 failover 되며, 클러스터가 커지거나 작아질 때마다 조정이 이루어집니다. 또한 커스텀 리소스의 변경 사항을 보고 변경 사항을 조정하기도 합니다.</p>
<p>Rook 은 여러분의 Pod 에 스토리지를 마운트하기 위해서 자동으로 Ceph-CSI driver 를 설정합니다.</p>
<p><img src="ceph_storage/ceph_storage_01.png" alt="ceph_storage_01" /></p>
<p><code>rook/ceph</code> 이미지는 클러스터를 관리하기 위한 모든 필요한 툴들이 포함되어 있습니다. placement group, crush map 같은 많은 Ceph 의 컨셉들은 숨겨져 있으며, 걱정할 필요가 없습니다. 대신에 Rook 은 물리적인 리소스, pool, volume, filesystem, bucket 에 대한 심플한 사용자 경험을 제공하빈다. 동시에, Ceph tool 을 통해 advanced 한 설정들을 적용할 수 도 있습니다.</p>
<p>Rook 은 golang 으로 개발되었고, Ceph 은 C++ 로 개발되었습니다. 우리는 이 조합이 최고를 제공한다고 믿습니다.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="admission-controller"><a class="header" href="#admission-controller">Admission Controller</a></h1>
<p>어드미션 컨트롤러는 쿠버네티스 리소스가 실제 반영되기 전 요청을 가로채어 인증 및 인가를 확인합니다.</p>
<p>Rook 이 커스텀 리소스 세팅과 함께 올바르게 설정되었는지 확인하기 위해 Rook 어드미션 컨트롤러를 활성화가 권장됩니다.</p>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<p>Rook 어드미션 컨트롤러를 배포하기 위해서, 설정을 자동화 하기 위한 헬퍼 스크립트를 제공합니다.</p>
<p>이 스크립트는 다음과 같은 작업을 하도록 돕습니다.</p>
<ul>
<li>cert-manager 를 사용해 certificate 를 생성</li>
<li>클러스터로부터 적절한 CA 번들로 ValidatingWebhookConfig 를 채워 생성</li>
</ul>
<p>아래 커맨드를 실행합니다.</p>
<pre><code class="language-sh">kubectl create -f deploy/examples/crds.yaml -f deploy/examples/common.yaml
tests/scripts/deploy_admission_controller.sh
</code></pre>
<p>Secret 이 배포되고 나면, operator 를 배포합니다.</p>
<pre><code class="language-sh">kubectl create -f deploy/examples/operator.yaml
</code></pre>
<p>완료되면, 오퍼레이터는 어드미션 컨트롤러 Deployment 를 자동으로 싱행하고, Rook 리소스에 대한 요청을 Webhook 이 인터셉트 하기 시작합니다.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="examples"><a class="header" href="#examples">Examples</a></h1>
<p>블록 디바이스, 공유 파일시스템, 그리고 오브젝트 스토리지를 쿠버네티스 네임스페이스에 제공하기 위해 Rook 과 Ceph 을 설정하는 방법은 다양합니다. 스토리지 셋업을 간단하게 하기 위한 여러가지 예제를 제공했지만, 여러분의 환경에 따라 다양한 튜닝과 설정이 필요할 수 있습니다.</p>
<p>rook/ceph 셋업 스펙 확인을 위해 <a href="https://github.com/rook/rook/blob/release-1.8/deploy/examples">example yaml files</a> 를 확인하세요.</p>
<h2 id="common-resources"><a class="header" href="#common-resources">Common Resources</a></h2>
<p>Rook 을 배포하기 위한 첫번째 스텝은 공통 리소스와 CRD 들을 생성하는 것입니다. 이 리소스들을 위한 설정은 대부분의 배포에서 동일합니다. <a href="https://github.com/rook/rook/blob/release-1.8/deploy/examples/crds.yaml">crds.yaml</a> 과 <a href="https://github.com/rook/rook/blob/release-1.8/deploy/examples/common.yaml">common.yaml</a> 을 사용합니다.</p>
<pre><code class="language-sh">kubectl create -f crds.yaml -f common.yaml
</code></pre>
<p>위 예제는 오퍼레이터와 Ceph 데몬들이 모두 같은 네임스페이스에 배포되는 것을 가정합니다. 만약 여러분이 다른 네임스페이스 배포를 원한다면, <code>common.yaml</code> 파일의 주석을 확인하세요.</p>
<h2 id="operator"><a class="header" href="#operator">Operator</a></h2>
<p>공통 리소스가 만들어졌다면, 다음 스텝은 오퍼레이터 Deployment 를 만드는 읿입니다. <a href="https://github.com/rook/rook/blob/release-1.8/deploy/examples/">이 디렉토리</a>에 여러 파일 스펙 예제들이 포함되어 있습니다.</p>
<ul>
<li><code>operator.yaml</code>: 가장 공통된 세팅의 프로덕션 배포
<ul>
<li><code>kubectl create -f operator.yaml</code></li>
</ul>
</li>
<li><code>operator-openshft.yaml</code>: Openshift 환경 배포에 최적화
<ul>
<li><code>oc create -f operator-openshift.yaml</code></li>
</ul>
</li>
</ul>
<p>오퍼레이터를 위한 설정들은 오퍼레이터 Deployment 에 환경변수로 설정됩니다. 각 세팅은 <a href="https://github.com/rook/rook/blob/release-1.8/deploy/examples/operator.yaml">operator.yaml</a> 에 기술되어 있습니다.</p>
<h2 id="cluster-crd"><a class="header" href="#cluster-crd">Cluster CRD</a></h2>
<p>오퍼레이터가 동작하기 시작하면, Ceph 스토리지 클러스터를 만들 수 있습니다. 이 커스텀 리소스는 오퍼레이터가 스토리지를 어떻게 운영할 것인지에 대한 크리티컬한 설정들이 포함됩니다. 클러스터를 설정하기 위한 여러가지 방법들을 충분히 이해하는 것이 중요합니다. 아래 예제들은 스토리지를 구성하기 위한 간단한 방법들을 보여줍니다.</p>
<ul>
<li><code>cluster.yaml</code>: 프로덕선 스토리지 클러스터의 공통 설정을 포함합니다. 적어도 세개 이상의 워커 노드가 필요합니다.</li>
<li><code>cluster-test.yaml</code>: 오직 하나의 노드만 필요한 테스트용 클러스터 설정입니다.</li>
<li><code>cluster-on-pvc.yaml</code>: Ceph mon 과 osd 들을 PV 로 사용하는 설정을 포함하고 있습니다. 클라우드 환겨이나 local PV 가 만들어져 있는 환경에 적합합니다.</li>
<li><code>cluster-external.yaml</code>: 외부 Ceph 클러스터를 연결하기 위한 설정입니다.</li>
<li><code>cluster-external-management.yaml</code>: 어드민 키를 사용해 원격으로 pool 을 만들고 오브젝트 스토어, 공유 파일시스템 등을 설정할 수 있는 설정입니다.</li>
<li><code>cluster-stretched.yaml</code>: 클러스터를 &quot;stretched&quot; 모들 생성할 수 있는 설정입니다. 세 개의 존에 5개의 노드, 그리고 두개의 존에 OSD 가 펼쳐집니다. <a href="ceph_storage//ceph_storage/cluster_crd.html#stretch-cluster">Stretch documentation</a> 문서를 참고하세요.</li>
</ul>
<p>더 많은 정보는 <a href="ceph_storage//ceph_storage/cluster_crd.html">Cluster CRD</a> 를 참고하세요.</p>
<h2 id="setting-up-consumable-storage"><a class="header" href="#setting-up-consumable-storage">Setting up consumable storage</a></h2>
<p>이제 Rook Ceph 클러스터 내에 블록, 오브젝트 스토리지 및 공유 파일 시스템이 사용 가능한 상태가 되었습니다. 이 스토리지들은 각각 CephBlockPool, CephFilesystem, CephObjectStore 리소스로 참조됩니다.</p>
<h3 id="block-devices"><a class="header" href="#block-devices">Block Devices</a></h3>
<p>Ceph 은 pod 에 raw 블록 디바이스를 제공할 수 있습니다. 아래의 각 예제들은 쿠버네티스 pod 에 블록 디바이스를 제공하는 storage class 를 셋업합니다. 이 storage class 는 <a href="http://docs.ceph.com/docs/master/rados/operations/pools/">pool</a>과 함께 저장 방식에 따라 정의됩니다.</p>
<ul>
<li><code>storageclass.yaml</code>: 이 예제는 3개의 레플리케이션을 가진 프로덕션 시나리오에서 사용되며, 적어도 세개의 워커 노드를 필요로 합니다. 쿠버네티스의 세 워커 노드에 레플리케이션되어 하나의 노드가 죽어도 데이터에 손실이 생기지 않습니다.</li>
<li><code>storageclass-ec.yaml</code>: Erasure coding 방식으로 설정합니다. Erasure coding 방식은 레플리케이션 방식보다 더 적은 데이터 공간을 차지하지만, 더 많은 컴퓨팅 파워를 요구합니다. 마찬가지로 적어도 세개의 워커 노드가 필요합니다. 문서를 참고하세요.</li>
<li><code>storageclass-test.yaml</code>: 1개의 레플리케이션을 요구하는 테스트용 시나리오를 위해 사용됩니다. 한개의 노드가 죽으면 데이터 손실로 이어지므로 프로덕션 레벨에서 사용해서는 안됩니다.</li>
</ul>
<p>storage class 는 드라이버에 따라 다른 서브디렉토리에 존재합니다.</p>
<ul>
<li><code>csi/rbd</code>: 블록 디바이스를 위한 CSI Driver 입니다. </li>
</ul>
<p>더 많은 정보는 <a href="ceph_storage//ceph_storage/block_pool_crd.html">Ceph Pool CRD</a> 문서를 참고하세요.</p>
<h3 id="shared-filesystem"><a class="header" href="#shared-filesystem">Shared Filesystem</a></h3>
<p>Ceph 파일시스템 (Cephfs) 는 하나 이상의 호스트(Pod)에 POSIX 호환의 폴더 마운트를 가능하게 해줍니다. NFS 나 CIFS 와 비슷합니다.</p>
<p>파일 스토리지는 여러 pool 을 포함하며, 여러 시나리오에서 아래 파일들을 사용합니다.</p>
<ul>
<li><code>filesystem.yaml</code>: 프로덕션 시나리오를 위한 3개의 레플리케이션, 세개 이상의 노드가 필요함</li>
<li><code>filesystem-ec.yaml</code>: 프로덕션 시나리오를 위한 Erasure coding, 세개 이상의 노드가 필요함</li>
<li><code>filesystem-test.yaml</code>: 테스트 시나리오를 위한 1개의 레플리케이션, 한개 노드가 필요</li>
</ul>
<p>CSI Driver 를 통해 다이나믹 프로비저닝이 가능합니다. 공유 파일시스템을 위한 storage class 는 <code>csi/cephfs</code> 디렉토리에 있습니다.</p>
<p><a href="ceph_storage//ceph_storage/shared_filesstem_crd.html">Shared Filesystem CRD</a> 문서를 참고하세요.</p>
<h3 id="object-storage"><a class="header" href="#object-storage">Object Storage</a></h3>
<p>Ceph 은 HTTP(s)-type 로 get/put/post/delete 오퍼레이션을 수행할 수 있는 오브젝트 스토리지를 제공합니다. AWS S3 스토리지와 비슷합니다.</p>
<p>오브젝트 스토리지는 여러 pool 을 포함하며, 여러 시나리오에서 각각 아래 예제 파일들을 사용할 수 있습니다.</p>
<ul>
<li><code>object.yaml</code>: 프로덕션을 위한 3개의 레플리케이션, 세개 이상의 노드 필요</li>
<li><code>object-openshift.yaml</code>: Openshift 환경을 위한 3개의 레플리케이션, 세개 이상의 노드 필요</li>
<li><code>object-ec.yaml</code>: 프로덕션을 위한 Erasure coding, 세개 이상의 노드 필요</li>
<li><code>object-test.yaml</code>: 테스트 환경을 위한 1개 레플리케이션, 한개 노드 필요</li>
</ul>
<p><a href="ceph_storage//ceph_storage/object_store_crd.html">Object Store CRD</a> 문서를 참고하세요.</p>
<h3 id="object-storage-user"><a class="header" href="#object-storage-user">Object Storage User</a></h3>
<ul>
<li><code>object-user.yaml</code>: 오브젝트 스토리지 유저를 생성하고, S3 API 를 위한 인증 정보를 생성합니다.</li>
</ul>
<h3 id="object-storage-bucket"><a class="header" href="#object-storage-bucket">Object Storage Bucket</a></h3>
<p>또한 Ceph 오퍼레이터는 기 존재하는 버킷에 권한을 부여하거나 신규 버킷을 다이나믹하게 생성할 수 있는 object store bucket provisioner 를 실행합니다.</p>
<ul>
<li><code>object-bucket-claim-retain.yaml</code>: Retain 상태로 StorageClass 를 참조하는 신규 버킷 요청을 생성합니다.</li>
<li><code>object-bucket-claim-delete.yaml</code>: Delete 상태로 StorageClass 를 참조하는 신규 버킷 요청을 생성합니다.</li>
<li><code>storageclass-bucket-retain.yaml</code>: Retain 상태로 Ceph 오브젝트 스토어, 리전을 정의하는 StorageClass 를 생성합니다.</li>
<li><code>storageclass-bucket-delete.yaml</code>: Delete 상태로 Ceph 오브젝트 스토어, 리전을 정의하는 StorageClass 를 생성합니다.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift"><a class="header" href="#openshift">Openshift</a></h1>
<blockquote>
<p><a href="https://rook.io/docs/rook/v1.8/ceph-openshift.html">생략</a></p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="block-storage"><a class="header" href="#block-storage">Block Storage</a></h1>
<p>블록 스토리지는 단일 Pod 에 스토리지 마운트를 지원합니다. 이 가이드는 Rook 으로 활성화된 PersistentVolume 을 어떻게 만들고 구성하는지 보여줍니다. </p>
<h2 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h2>
<p><a href="ceph_storage//quickstart.html">Quickstart</a> 가이드에 따라 Rook 클러스터가 구축된 상황을 가정합니다.</p>
<h2 id="provision-storage"><a class="header" href="#provision-storage">Provision Storage</a></h2>
<p>Rook 이 스토리지를 프로비저닝 하기 전에, <code>StorageClass</code> 와 <code>CephBlockPool</code> 커스텀 리소스가 생성되어 있어야 합니다. 이렇게 하면 PersistentVolume 을 프로비저닝 할 때 쿠버네티스가 Rook 과 상호작용 할 수 있습니다.</p>
<blockquote>
<p><strong>NOTE</strong>: 예제는 세개의 다른 노드에 있는 OSD 와 함께 노드당 적어도 1개 이상의 OSD 가 필요합니다. </p>
</blockquote>
<p>아래의 <code>stoargeclass.yaml</code> 에 정의되어 있는 <code>StorageClass</code> 를 저장합니다. </p>
<pre><code class="language-yaml">apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 3
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block
# Change &quot;rook-ceph&quot; provisioner prefix to match the operator namespace if needed
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
    # clusterID is the namespace where the rook cluster is running
    clusterID: rook-ceph
    # Ceph pool into which the RBD image shall be created
    pool: replicapool

    # (optional) mapOptions is a comma-separated list of map options.
    # For krbd options refer
    # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
    # For nbd options refer
    # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
    # mapOptions: lock_on_read,queue_depth=1024

    # (optional) unmapOptions is a comma-separated list of unmap options.
    # For krbd options refer
    # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
    # For nbd options refer
    # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
    # unmapOptions: force

    # RBD image format. Defaults to &quot;2&quot;.
    imageFormat: &quot;2&quot;

    # RBD image features. Available for imageFormat: &quot;2&quot;. CSI RBD currently supports only `layering` feature.
    imageFeatures: layering

    # The secrets contain Ceph admin credentials.
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

    # Specify the filesystem type of the volume. If not specified, csi-provisioner
    # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
    # in hyperconverged settings where the volume is mounted on the same node as the osds.
    csi.storage.k8s.io/fstype: ext4

# Delete the rbd volume when a PVC is deleted
reclaimPolicy: Delete

# Optional, if you want to add dynamic resize for PVC. Works for Kubernetes 1.14+
# For now only ext3, ext4, xfs resize support provided, like in Kubernetes itself.
allowVolumeExpansion: true
</code></pre>
<p>&quot;rook-ceph&quot; 외에 다른 네임스페이스에 Rook 오퍼레이터를 배포했다면, 프로비저너의 프리픽스를 해당 네임스페이스로 변경해야 합니다. 예를 들어, Rook 오퍼레이터가 설치된 네임스페이스가 &quot;my-namespace&quot; 라면 프로비저너 스펙은 &quot;my-namespace.rbd.csi.ceph.com&quot; 이 됩니다.</p>
<p>storage class 를 생성합니다.</p>
<pre><code class="language-sh">kubectl create -f deploy/examples/csi/rbd/storageclass.yaml
</code></pre>
<blockquote>
<p><strong>NOTE</strong>: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain">쿠버네티스 문서</a> 에서 언급되었다시피, <code>Retain</code> reclaim policy 를 사용하게 되면, <code>PersistentVolume</code> 이 삭제되더라도 Ceph RBD 볼륨은 삭제되지 않습니다. <code>rbd rm</code> 을 통해 클린업이 필요합니다.</p>
</blockquote>
<h2 id="consume-the-storage-wordpress-sample"><a class="header" href="#consume-the-storage-wordpress-sample">Consume the storage: Wordpress sample</a></h2>
<p>Rook 에 의해 프로비저닝 된 블록 스토리지를 사용하여 wordpress 와 mysql 어플리케이션을 생성합니다. 
두 어플리케이션은 Rook 으로 프로비저닝 된 블록 볼륨을 사용합니다.</p>
<p><code>deploy/examples</code> 폴더 내의 mysql 과 wordpress 를 배포합니다.</p>
<pre><code class="language-sh">kubectl create -f mysql.yaml
kubectl create -f wordpress.yaml
</code></pre>
<p>두 어플리케이션은 각각 블록 볼륨을 마운트하며, 아래 커맨드로 쿠버네티스 PersistentVolumeClaim 이 생성된 것을 확인할 수 있습니다.</p>
<pre><code class="language-sh">kubectl get pvc
</code></pre>
<pre><code class="language-sh">NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
mysql-pv-claim   Bound     pvc-95402dbc-efc0-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
wp-pv-claim      Bound     pvc-39e43169-efc1-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
</code></pre>
<p>두 Pod 이 <code>Running</code> 상태가 되면, 여러분의 브라우저에서 접근할 포트를 확인합니다.</p>
<pre><code class="language-sh">kubectl get svc wordpress
</code></pre>
<pre><code class="language-sh">NAME        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
wordpress   10.3.0.155   &lt;pending&gt;     80:30841/TCP   2m
</code></pre>
<p>이제 wordpress 어플리케이션이 실행하는 것을 확인할 수 있습니다.
minikube 를 사용한다면, 아래 커맨드로 wordpress URL 을 확인할 수 있습니다.</p>
<pre><code class="language-sh">echo http://$(minikube ip):$(kubectl get service wordpress -o jsonpath='{.spec.ports[0].nodePort}')
</code></pre>
<blockquote>
<p><strong>NOTE</strong>: vagrant 환경에서 실행한다면, wordpress 어플리케이션을 확인할 external ip 가 존재하지 않습니다. 쿠버네티스 클러스터 안에서 <code>CLUSTER-IP</code> 를 통해서만 wordpress 에 접근할 수 있습니다.</p>
</blockquote>
<h2 id="consume-the-storage-toolbox"><a class="header" href="#consume-the-storage-toolbox">Consume the storage: Toolbox</a></h2>
<p>위에서 만든 pool 로, 블록 이미지를 만들고 pod 에 직접적으로 마운트 할 수 있습니다. <a href="ceph_storage//ceph_tools/direct_tools.html">Direct Block Tools</a> 를 참고하세요.</p>
<h2 id="teardown-1"><a class="header" href="#teardown-1">Teardown</a></h2>
<p>위 데모에서 생성한 모든 리소스를 삭제하려면 아래 커맨드를 입력합니다.</p>
<pre><code class="language-sh">kubectl delete -f wordpress.yaml
kubectl delete -f mysql.yaml
kubectl delete -n rook-ceph cephblockpools.ceph.rook.io replicapool
kubectl delete storageclass rook-ceph-block
</code></pre>
<h2 id="advanced-example-erasure-coded-block-storage"><a class="header" href="#advanced-example-erasure-coded-block-storage">Advanced Example: Erasure Coded Block Storage</a></h2>
<p>RBD 를 erasure code pool 로 사용하고 싶다면, OSD 의 <code>storageType</code> 이 <code>bluestore</code> 로 설정되어 있어야 합니다. 또한 마운트가 가능하려면 커널이 <code>4.11</code> 이상이어야 합니다.</p>
<p><strong>NOTE</strong>: 예제는 세 개 이상의 OSD 가 필요하며, 각 OSD 는 서로 다른 노드에 존재해야 합니다.</p>
<p>OSD 는 <code>failureDomain</code> 이 <code>host</code> 로 설정되어 있고, <code>erasureCoded</code> 청크 설정이 적어도 세 개의 다른 OSD (2 <code>dataChunks</code> + 1 <code>codingChunks</code>) 가 필요하기 문에, OSD 는 서로 다른 노드에 존재하고 있어야 합니다.</p>
<p>erasure coded pool 의 사용을 위해서는 두개의 pool 을 생성해야 합니다.</p>
<h3 id="erasure-coded-csi-driver"><a class="header" href="#erasure-coded-csi-driver">Erasure Coded CSI Driver</a></h3>
<p>Erasure coded pool 을 사용하려면 <code>storageclass-ec.yaml</code> 에 <code>dataPool</code> 파라미터가 설정되어 있어야 합니다. RBD 이미지의 데이터로 사용됩니다.</p>
<h3 id="erasure-coded-flex-driver"><a class="header" href="#erasure-coded-flex-driver">Erasure Coded Flex Driver</a></h3>
<p>Erasure coded pool 을 사용하려면 <code>storageclass-ec.yaml</code> 에 <code>dataBlockPool</code> 파라미터가 설정되어 있어야 합니다. RBD 이미지의 데이터로 사용됩니다.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="object-storage-1"><a class="header" href="#object-storage-1">Object Storage</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="shared-filesystem-1"><a class="header" href="#shared-filesystem-1">Shared Filesystem</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ceph-dashboard-1"><a class="header" href="#ceph-dashboard-1">Ceph Dashboard</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prometheus-monitoring"><a class="header" href="#prometheus-monitoring">Prometheus Monitoring</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cluster-crd-1"><a class="header" href="#cluster-crd-1">Cluster CRD</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="block-pool-crd"><a class="header" href="#block-pool-crd">Block Pool CRD</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="object-store-crd"><a class="header" href="#object-store-crd">Object Store CRD</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="object-multisite-crds"><a class="header" href="#object-multisite-crds">Object Multisite CRDs</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="object-bucket-claim"><a class="header" href="#object-bucket-claim">Object Bucket Claim</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="object-store-user-crd"><a class="header" href="#object-store-user-crd">Object Store User CRD</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bucket-notifications"><a class="header" href="#bucket-notifications">Bucket Notifications</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="shared-filesystem-crd"><a class="header" href="#shared-filesystem-crd">Shared Filesystem CRD</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nfs-crd"><a class="header" href="#nfs-crd">NFS CRD</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ceph-csi"><a class="header" href="#ceph-csi">Ceph CSI</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rbd-mirroring"><a class="header" href="#rbd-mirroring">RBD Mirroring</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="failover-and-failback"><a class="header" href="#failover-and-failback">Failover and Failback</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="snapshots"><a class="header" href="#snapshots">Snapshots</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="volume-clone"><a class="header" href="#volume-clone">Volume clone</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="client-crd"><a class="header" href="#client-crd">Client CRD</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rbdmirror-crd"><a class="header" href="#rbdmirror-crd">RBDMirror CRD</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="filesystemmirror-crd"><a class="header" href="#filesystemmirror-crd">FilesystemMirror CRD</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="subvolumegroup-crd"><a class="header" href="#subvolumegroup-crd">SubVolumeGroup CRD</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="key-management-system"><a class="header" href="#key-management-system">Key Management System</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration"><a class="header" href="#configuration">Configuration</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="upgrades"><a class="header" href="#upgrades">Upgrades</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cleanup"><a class="header" href="#cleanup">Cleanup</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="helm-charts"><a class="header" href="#helm-charts">Helm Charts</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ceph-operator"><a class="header" href="#ceph-operator">Ceph Operator</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ceph-cluster"><a class="header" href="#ceph-cluster">Ceph Cluster</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h1>
<p>Rook 클러스터를 운영하면서 트러블슈팅을 돕기 위한 몇몇 팁들입니다. 이 페이지에서 찾은 해결방안으로 시도해도 문제가 해결되지 않은 경우, <a href="https://slack.rook.io/">Rook Slack 가입</a> 후 General 채널에서 도움을 요청하세요. </p>
<h2 id="ceph"><a class="header" href="#ceph">Ceph</a></h2>
<p>Ceph 에 관련된 이슈들은 <a href="/ceph_tools/common_issues.html">Ceph Common Issues</a> 페이지를 참고하세요.</p>
<h2 id="troubleshooting-techniques"><a class="header" href="#troubleshooting-techniques">Troubleshooting Techniques</a></h2>
<p>쿠버네티스 상태와 로그는 Rook 클러스터 이슈를 확인하는데 필요한 중요 리소스입니다.</p>
<h2 id="kubernetes-tools"><a class="header" href="#kubernetes-tools">Kubernetes Tools</a></h2>
<p>쿠버네티스 상태는 클러스터가 정상 동작하지 않을 때 가장 처음 확인해야할 리소스입니다. 아래 방법들이 상태를 확인하는데 도움을 줄 수 있습니다.</p>
<ul>
<li>Rook pod status
<ul>
<li><code>kubectl get po -n &lt;cluster-namespace&gt; -o wide</code></li>
</ul>
</li>
<li>Logs for Rook pods
<ul>
<li>오퍼레이터 로그: <code>kubectl logs -n &lt;cluster-namespace&gt; -l app=&lt;storage-backend-operator&gt;</code></li>
<li>특정 Pod 에 대한 로그: <code>kubectl logs -n &lt;cluster-namespace&gt; &lt;pod-name&gt;</code>, 혹은 pod label (ex. mon1) 로 조회: <code>kubectl logs -n &lt;cluster-namespace&gt; -l &lt;label-matcher&gt;</code></li>
<li>PVC 가 왜 마운트에 실패했는지에 대한 로그: 
<ul>
<li>노드에 연결하고, kubelet 로그를 확인합니다: <code>journalctl -u kubelet</code></li>
</ul>
</li>
<li>여러 컨테이너가 동작하는 Pod
<ul>
<li>모든 컨테이너에 대해서: <code>kubectl -n &lt;cluster-namespace&gt; logs &lt;pod-name&gt; --all-containers</code></li>
<li>특정 컨테이너에 대해서: <code>kubectl -n &lt;cluster-namespace&gt; logs &lt;pod-name&gt; -c &lt;container-name&gt;</code></li>
</ul>
</li>
<li>더 이상 running 상태가 아닌 Pod 로그 확인: <code>kubectl -n &lt;cluster-namespace&gt; logs --previous &lt;pod-name&gt;</code></li>
</ul>
</li>
</ul>
<p>몇몇 Pod 들은 initContainers 스펙을 가지고 있으며, pod 내에서 다른 컨테이너의 로그를 확인해야 할 수 도 있습니다.</p>
<ul>
<li><code>kubectl -n &lt;namespace&gt; logs &lt;pod-name&gt; -c &lt;container-name&gt;</code></li>
<li>다른 Rook 컴포넌트들: <code>kubectl -n &lt;cluster-namespace&gt; get all</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ceph-tools"><a class="header" href="#ceph-tools">Ceph Tools</a></h1>
<p>Rook 은 클러스터 운영을 위한 여러 툴들과 트러블슈팅 문서를 제공합니다.</p>
<ul>
<li><a href="ceph_tools/toolbox.html">Toolbox</a>: 스토리지 클러스터의 트러블슈팅을 위한 모든 툴들을 실행할 수 있는 pod</li>
<li><a href="ceph_tools/common_issues.html">Common Issues</a>: 공통 이슈와 해결법</li>
<li><a href="ceph_tools/osd_management.html">OSD Management</a>: 스토리지를 추가하거나 삭제할 때의 Ceph OSD 에 대한 공통 설정 이슈</li>
<li><a href="ceph_tools/direct_tools.html">Direct Tools</a>: 직접 블록 및 파일 스토리지를 마운트하여 테스트 하기 위한 툴</li>
<li><a href="ceph_tools/advanced_configuration.html">Advanced Configuration</a>: 클러스터를 설정하기 위한 팁과 트릭들</li>
<li><a href="ceph_tools/openshift_common_issues.html">Openshift Common Issues</a>: Openshift 클러스터를 위한 트러블슈팅 팁</li>
<li><a href="ceph_tools/disaster_recovery.html">Disaster Recovery</a>: mon 들이 quorum 을 잃어버렸을 때 최악의 시나리오를 위한 recover 스텝</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="toolbox"><a class="header" href="#toolbox">Toolbox</a></h1>
<p>Rook toolbox 는 Rook 의 디버깅과 테스트를 위한 툴 컨테이너입니다. toolbox 는 CentOS 를 기반으로 만들어져 있어, <code>yum</code> 패키지 매니저를 사용하여 손쉽게 추가 패키지 설치가 가능합니다.</p>
<p>toolbox 는 두가지 모드로 실행할 수 있습니다.</p>
<ol>
<li><a href="ceph_tools/toolbox.html#interactive-toolbox">Interactive</a>: Shell 에서 Ceph 에 연결하여 커맨드를 실행할 수 있는 toolbox pod 을 시작합니다.</li>
<li><a href="ceph_tools/toolbox.html#toolbox-job">One-time job</a>: Ceph 커맨드를 실행하여 job 로그를 가져옵니다.</li>
</ol>
<blockquote>
<p>Prerequisite: toolbox 를 실행하기 전에, Rook 클러스터가 배포되어 있어야 합니. (<a href="ceph_tools//quickstart.html">Quickstart Guide</a> 를 참고하세요.)</p>
</blockquote>
<h2 id="interactive-toolbox"><a class="header" href="#interactive-toolbox">Interactive Toolbox</a></h2>
<p>Rook toolbox 는 쿠버네티스 클러스터 안에서 Deployment 로 배포할 수 있으며, Pod 내에서 Ceph 커맨드를 실행할 수 있습니다.</p>
<p>rook-ceph-tools pod 을 배포합니다.</p>
<pre><code class="language-sh">kubectl create -f deploy/examples/toolbox.yaml
</code></pre>
<p>toolbox pod 이 <code>Runinng</code> 상태가 될 때까지 기다립니다.</p>
<pre><code class="language-sh">kubectl -n rook-ceph rollout status deploy/rook-ceph-tools
</code></pre>
<p>rook-ceph-tools 가 <code>Running</code> 상태이면, Pod 에 연결해 커맨드를 실행할 수 있습니다.</p>
<p>*<em>Example</em></p>
<ul>
<li><code>ceph status</code></li>
<li><code>ceph osd status</code></li>
<li><code>ceph df</code></li>
<li><code>rados df</code></li>
</ul>
<p>작업이 끝낫다면, Deployment 를 삭제합니다.</p>
<pre><code class="language-sh">kubectl -n rook-ceph delete deploy/rook-ceph-tools
</code></pre>
<h2 id="toolbox-job"><a class="header" href="#toolbox-job">Toolbox Job</a></h2>
<p>커맨드를 한번만 실행해서 결과를 얻고 싶다면, 쿠버네티스 Job 을 사용할 수 있습니다. toolbox job 은 job 스펙에 명시되어 있는 스크립트를 실행합니다. 스크립트는 Bash 스크립트로 작성합니다.</p>
<p>아래 예제에서, job 이 생성되면 <code>ceph status</code> 커맨드가 실행됩니다.</p>
<pre><code class="language-sh">kubectl create -f deploy/examples/toolbox-job.yaml
</code></pre>
<p>job 이 완료되면, 아래 커맨드로 스크립트의 결과를 화인합니다.</p>
<pre><code class="language-sh">kubectl -n rook-ceph logs -l job-name=rook-ceph-toolbox-job
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="common-issues-1"><a class="header" href="#common-issues-1">Common Issues</a></h1>
<p>대부분의 이슈들은 간단한 몇 문장으로 요약할 수 없습니다. 몇가지 문제가 아래 리스트에 정리되어 있으며, Rook 설정에 따라 적용할 수 없다는 점을 기억하세요. </p>
<p>이 페이지에서 찾은 해결방안으로 시도해도 문제가 해결되지 않은 경우, <a href="https://slack.rook.io/">Rook Slack 가입</a> 후 General 채널에서 도움을 요청하세요. </p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="ceph_tools/common_issues.html#table-of-contents">Table of Contents</a></li>
<li><a href="ceph_tools/common_issues.html#%ED%8A%B8%EB%9F%AC%EB%B8%94%EC%8A%88%ED%8C%85-%ED%85%8C%ED%81%AC%EB%8B%89">트러블슈팅 테크닉</a></li>
<li><a href="ceph_tools/common_issues.html#%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EA%B0%80-%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%9A%94%EC%B2%AD%EC%97%90-%EC%9D%91%EB%8B%B5%ED%95%98%EC%A7%80-%EB%AA%BB%ED%95%A8">클러스터가 서비스 요청에 응답하지 못함</a></li>
<li><a href="ceph_tools/common_issues.html#mon-%EB%A7%8C-%EB%B0%B0%ED%8F%AC%EB%90%A8">Mon 만 배포됨</a></li>
<li><a href="ceph_tools/common_issues.html#pvc-%EA%B0%80-pending-%EC%83%81%ED%83%9C%EB%A1%9C-%EC%A7%80%EC%86%8D%EB%90%A8">PVC 가 Pending 상태로 지속됨</a></li>
<li><a href="ceph_tools/common_issues.html#osd-pod-%EB%93%A4%EC%9D%B4-%EC%8B%9C%EC%9E%91%EB%90%98%EC%A7%80-%EC%95%8A%EC%9D%8C">OSD Pod 들이 시작되지 않음</a></li>
<li><a href="ceph_tools/common_issues.html#osd-pod-%EB%93%A4%EC%9D%B4-%EB%94%94%EB%B0%94%EC%9D%B4%EC%8A%A4%EC%97%90-%EC%83%9D%EC%84%B1%EB%90%98%EC%A7%80-%EC%95%8A%EC%9D%8C">OSD Pod 들이 디바이스에 생성되지 않음</a></li>
<li><a href="ceph_tools/common_issues.html#%EB%85%B8%EB%93%9C-%EB%A6%AC%EB%B6%80%ED%8C%85-%EC%9D%B4%ED%9B%84-%EB%85%B8%EB%93%9C%EA%B0%80-hang-%EC%83%81%ED%83%9C%EB%A1%9C-%EC%A7%80%EC%86%8D%EB%90%A8">노드 리부팅 이후 노드가 Hang 상태로 지속됨</a></li>
<li><a href="ceph_tools/common_issues.html#%EC%97%AC%EB%9F%AC-cephfs-%EB%A5%BC-%EC%BB%A4%EB%84%90-47-%EB%AF%B8%EB%A7%8C-%EB%B2%84%EC%A0%84%EC%97%90%EC%84%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%A0%A4%EA%B3%A0-%ED%95%A8">여러 Cephfs 를 커널 4.7 미만 버전에서 사용하려고 함</a></li>
<li><a href="ceph_tools/common_issues.html#%EB%AA%A8%EB%93%A0-ceph-%EB%8D%B0%EB%AA%AC%EB%93%A4%EC%9D%98-%EB%A1%9C%EA%B7%B8-%EB%A0%88%EB%B2%A8%EC%9D%84-debug-%EB%A1%9C-%EB%B3%80%EA%B2%BD">모든 Ceph 데몬들의 로그 레벨을 Debug 로 변경</a></li>
<li><a href="ceph_tools/common_issues.html#%ED%8A%B9%EC%A0%95-ceph-%EB%8D%B0%EB%AA%AC%EC%97%90-%EB%8C%80%ED%95%B4-file-%EB%A1%9C-%EB%A1%9C%EA%B7%B8%EB%A5%BC-%EC%93%B0%EB%8F%84%EB%A1%9D-%EB%B3%80%EA%B2%BD">특정 Ceph 데몬에 대해 file 로 로그를 쓰도록 변경</a></li>
<li><a href="ceph_tools/common_issues.html#rbd-%EB%94%94%EB%B0%94%EC%9D%B4%EC%8A%A4%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EC%9B%8C%EC%BB%A4-%EB%85%B8%EB%93%9C%EA%B0%80-hang-%EC%83%81%ED%83%9C%EB%A1%9C-%EC%A7%80%EC%86%8D%EB%90%A8">RBD 디바이스를 사용하는 워커 노드가 Hang 상태로 지속됨</a></li>
<li><a href="ceph_tools/common_issues.html#too-few-pgs-per-osd-%EB%A9%94%EC%8B%9C%EC%A7%80%EA%B0%80-%EB%82%98%ED%83%80%EB%82%A8">Too few PGs per OSD 메시지가 나타남</a></li>
<li><a href="ceph_tools/common_issues.html#lv-%EB%B0%B1%EC%97%94%EB%93%9C-pvc-%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-osd-%EC%97%90%EC%84%9C-lvm-%EB%A9%94%ED%83%80%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B0%80-%EC%86%90%EC%83%81%EB%90%A8">LV 백엔드 PVC 를 사용하는 OSD 에서 LVM 메타데이터가 손상됨</a></li>
<li><a href="ceph_tools/common_issues.html#osd-prepare-job-%EC%9D%B4-to-low-aio-max-nr-%EB%A1%9C-%EC%8B%A4%ED%8C%A8%ED%95%A8">OSD Prepare Job 이 to low aio-max-nr 로 실패함</a></li>
<li><a href="ceph_tools/common_issues.html#%EC%98%88%EC%83%81%ED%95%98%EC%A7%80-%EC%95%8A%EC%9D%80-%ED%8C%8C%ED%8B%B0%EC%85%98%EC%9D%B4-%EC%83%9D%EC%84%B1%EB%90%A8">예상하지 않은 파티션이 생성됨</a></li>
</ul>
<h2 id="트러블슈팅-테크닉"><a class="header" href="#트러블슈팅-테크닉">트러블슈팅 테크닉</a></h2>
<p>클러스터 내 이슈를 확인하기 위해서 크게 두 가지를 확인해 볼 수 있습니다.</p>
<ol>
<li><a href="ceph_tools//common_issues.html">쿠버네티스 상태 및 로그</a></li>
<li>Ceph 클러스터 상태 (다음 섹션의 <a href="ceph_tools/common_issues.html#ceph-tools">Ceph tools</a>)</li>
</ol>
<h3 id="ceph-tools-1"><a class="header" href="#ceph-tools-1">Ceph Tools</a></h3>
<p>Pod 들이 정상적으로 Running 상태임을 확인했다면, 스토리지 컴포넌트의 상태를 확인하기 위해 Ceph tools 를 사용합니다. Rook toolbox 를 사용하거나, 이미 Running 중인 Rook pod 안에서 확인할 수 있습니다.</p>
<ul>
<li>왜 PVC 가 마운트에 실패했는지에 대한 노드의 로그 확인</li>
<li><a href="ceph_tools/advanced_configuration.html#log-collection">로그 수집</a> 섹션에서 로그 확인을 위한 스크립트 확인 </li>
<li>다른 방안:
<ul>
<li>mon 이 quorum 내에 있는지 확인: <code>kubectl -n &lt;cluster-namespace&gt; get configmap rook-ceph-mon-endpoints -o yaml | grep data</code></li>
</ul>
</li>
</ul>
<h4 id="tools-in-the-rook-toolbox"><a class="header" href="#tools-in-the-rook-toolbox">Tools in the Rook Toolbox</a></h4>
<p><a href="ceph_tools/toolbox.html">rook-ceph-tools pod</a> 은 Ceph tools 를 실행하기 위한 환경을 제공합니다. 일단 Pod 이 실행되고 나면, 현재 클러스터 상태를 확인하기 위한 Ceph 커맨드를 실행합니다.</p>
<pre><code class="language-sh">kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot; -o jsonpath='{.items[*].metadata.name}') bash
</code></pre>
<h3 id="ceph-commands"><a class="header" href="#ceph-commands">Ceph Commands</a></h3>
<p>아래와 같은 커맨드로 현재 Ceph 클러스터의 상태를 확인할 수 있습니다.</p>
<ul>
<li><code>ceph status</code></li>
<li><code>ceph osd status</code></li>
<li><code>ceph osd df</code></li>
<li><code>ceph osd utilization</code></li>
<li><code>ceph osd pool stats</code></li>
<li><code>ceph osd tree</code></li>
<li><code>ceph pg stat</code></li>
</ul>
<p>처음 두 커맨드는 전체적인 클러스터 상태를 나타냅니다. 정상적인 상태의 클러스터는 <code>HEALTH_OK</code> 이지만, <code>HEALTH_WARN</code> 상태에서도 클러스터는 정상적으로 동작합니다. 하지만 이 상태는 모든 디스크 I/O 가 불가능한 상태인 <code>HEALTH_ERROR</code> 상태로 빠질 수 있는 상태입니다. <code>HEALTH_WARN</code> 상태가 관측된다면, 클러스터가 에러 상태로 빠지지 않도록 조치를 취해야 합니다.</p>
<p>Ceph 의 리소스들을 수정할 수 있는 여러 Ceph 서브커맨드들이 있지만, 이 문서에서는 다루지 않습니다. 클러스터 상태에 대한 더 많은 정보는, <a href="https://docs.ceph.com/">Ceph 문서</a> 를 확인하세요. <a href="ceph_tools/advanced_configuration.html">Advanced Configuration 섹션</a> 에서, 도움이 될 만한 여러 힌트들을 제공하고 있습니다. </p>
<h2 id="클러스터가-서비스-요청에-응답하지-못함"><a class="header" href="#클러스터가-서비스-요청에-응답하지-못함">클러스터가 서비스 요청에 응답하지 못함</a></h2>
<h3 id="증상"><a class="header" href="#증상">증상</a></h3>
<ul>
<li><code>ceph</code> 커맨드가 행 상태로 실행되지 않음</li>
<li>PersistentVolume 이 생성되지 않음</li>
<li>다수의 slow request </li>
<li>다수의 stuck request</li>
<li>한 개 이상의 mon 들이 계속 재시작됨</li>
</ul>
<h3 id="확인"><a class="header" href="#확인">확인</a></h3>
<p>현재 Ceph 상태를 확인하기 위해 <a href="ceph_tools/toolbox.html">rook-ceph-tools pod</a> 을 생성합니다. 아래 상화에서, <code>ceph status</code> 커맨드는 행 상태로 실행되지 않습니다. CTRL+C 로 멈춰야 합니다.</p>
<pre><code class="language-sh">kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status

ceph status
^CCluster connection interrupted or timed out
</code></pre>
<p>다른 확인 방법은 mon pod 들이 주기적으로 재시작 되는 것을 확인하는 것입니다. 아래 예시에서 'mon107' pod 은 16분 전 재시작되었습니다.</p>
<pre><code class="language-sh">kubectl -n rook-ceph get all -o wide --show-all
</code></pre>
<pre><code class="language-sh">NAME                                 READY     STATUS    RESTARTS   AGE       IP               NODE
po/rook-ceph-mgr0-2487684371-gzlbq   1/1       Running   0          17h       192.168.224.46   k8-host-0402
po/rook-ceph-mon107-p74rj            1/1       Running   0          16m       192.168.224.28   k8-host-0402
rook-ceph-mon1-56fgm                 1/1       Running   0          2d        192.168.91.135   k8-host-0404
rook-ceph-mon2-rlxcd                 1/1       Running   0          2d        192.168.123.33   k8-host-0403
rook-ceph-osd-bg2vj                  1/1       Running   0          2d        192.168.91.177   k8-host-0404
rook-ceph-osd-mwxdm                  1/1       Running   0          2d        192.168.123.31   k8-host-0403
</code></pre>
<h3 id="해결방법"><a class="header" href="#해결방법">해결방법</a></h3>
<p>이 증상은 하나 이상의 Ceph 데몬이 적절한 클러스터 설정으로 구성되지 않았을 떄 발생합니다. 일반적으로 클러스터 CRD 에서 <code>dataDirHostPath</code> 값을 지정하지 않은 결과입니다.</p>
<p><code>dataDirHostPath</code> 설정은 호스트에 Ceph 데몬을 위한 설정파일과 데이터를 저장하는 경로를 지정합니다. <code>/var/lib/rook</code> 와 같이 지정되며, Cluster CRD 를 다시 배포하거나 Ceph 데몬 (MON, MGR, ODS, RGW) 을 재시작하면 문제 해결에 도움이 됩니다. Ceph 데몬이 재시작 되고 나면, <a href="ceph_tools/toolbox.html">rook-tool pod</a> 를 재시작할 수 있습니다.</p>
<h2 id="mon-만-배포됨"><a class="header" href="#mon-만-배포됨">Mon 만 배포됨</a></h2>
<h3 id="증상-1"><a class="header" href="#증상-1">증상</a></h3>
<ul>
<li>Rook 오퍼레이터가 동작 중임</li>
<li>하나의 mon 이 시작되거나, 매우 느리게 시작됨 (몇 분 뒤에도)</li>
<li>crash-collector pod 이 동작하지 않음</li>
<li>CSI 드라이버 외에 mgr, osd 등 다른 컴포넌트들이 생성되지 않음 </li>
</ul>
<h3 id="확인-1"><a class="header" href="#확인-1">확인</a></h3>
<p>오퍼레이터가 클러스터를 시작할 때, 모든 mon 을 실행하기 전에 하나의 mon 을 배포하여 체크합니다. 첫번째 mon 이 정상적이지 않으면, 오퍼레이터는 정상 상태가 될때까지 체크합니다. 첫번째 mon 이 정상적으로 시작되고 나서야 두번째, 세번째 mon 이 시작됩니다. 시작된 mon 들은 바로 쿼럼을 형성하지는 않으며, 오케스트레이션은 블로킹 된 상태가 됩니다.</p>
<p>crash-collector pod 은 mon 들이 처음 쿼럼을 형성할 때 까지 시작되지 않습니다.</p>
<p>mon 이 쿼럼을 형성하지 못하는 이유는 여러가지가 있을 수 있습니다.</p>
<ul>
<li>오퍼레이터 pod 이 mon pod (들)과 연결을 맺지 못함. 네트워크 설정이 잘못되어 있을 수 있습니다.</li>
<li>하나 이상의 pod 이 실행 상태이지만, 쿼럼을 형성하지 못한다는 로그가 오퍼레이터에 존재 </li>
<li>mon 이 이전 설치 떄 사용한 설정을 사용하고 있음. 이전 클러스터를 <a href="ceph_tools//ceph_storage/cleanup.html#delete-the-data-on-hosts">클린업 하기 위한 가이드</a> 를 참고합니다.</li>
<li>방화벽이 mon 이 쿼럼을 형성하기 위한 포트를 막고 있음. 6789, 3300 포트가 열려있는지 확인합니다. 자세한 정보는 <a href="https://docs.ceph.com/en/latest/rados/configuration/network-config-ref/">Ceph 네트워크 가이드</a>를 참고합니다.</li>
<li>컴포넌트간 MTU 가 일치하지 않을 수 있습니다. 쿠버네티스 CNI 플러그인이나 호스트가 점포 프레임 (MTU 9000) 이 활성화 되어 있으면, Ceph 은 네트워크 대역폭을 높이기 위해 더 큰 패킷을 사용합니다. 네트워크를 사용하는 다른 부분이 점포 프레임을 지원하지 않으면, 예상치 못한 패킷 유실이 있을 수 있습니다.</li>
</ul>
<h4 id="오퍼레이터가-mon-에-연결하지-못함"><a class="header" href="#오퍼레이터가-mon-에-연결하지-못함">오퍼레이터가 mon 에 연결하지 못함</a></h4>
<p>먼저 오퍼레이터가 mon 들에 접근할 수 있는지 로그를 확인합니다.</p>
<pre><code class="language-bash">kubectl -n rook-ceph logs -l app=rook-ceph-operator
</code></pre>
<p>오퍼레이터가 mon 에 연결을 시도하다가 타임아웃이 된 로그를 확인할 수 있습니다. 마지막 커맨드는 <code>ceph mon_status</code> 였고, 5분 뒤 타임아웃 메시지가 나타납니다.</p>
<pre><code class="language-bash">2018-01-21 21:47:32.375833 I | exec: Running command: ceph mon_status --cluster=rook --conf=/var/lib/rook/rook-ceph/rook.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/442263890
2018-01-21 21:52:35.370533 I | exec: 2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300
2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300
2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out
2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out
[errno 110] error connecting to the cluster
</code></pre>
<p>인증 에러로 로그가 나타날 수 도 있지만, 실제 이슈는 타임아웃일 수 있습니다.</p>
<h4 id="해결방법-1"><a class="header" href="#해결방법-1">해결방법</a></h4>
<p>오퍼레이터 로그에서 타임아웃 로그를 봤다면, mon pod 들이 Running 상태인지 확인합니다. mon pod 들이 모두 Running 상태라면, 오퍼레이터 pod 과 mon pod 간의 네트워크를 확인합니다. 대부분의 경우 CNI 플러그인이 정상적으로 설정되지 않을 때 나타납니다.</p>
<p>네트워크 연결을 확잏나려면, </p>
<ul>
<li>mon 엔드포인트를 얻습니다.</li>
<li>오퍼레이터 pod 에서 curl 툴로 mon 연결을 확인합니다.</li>
</ul>
<p>예를들어, 아래 커맨드는 오퍼레이터에서 첫번째 mon 을 curl 로 확인합니다.</p>
<pre><code class="language-bash">kubectl -n rook-ceph exec deploy/rook-ceph-operator -- curl $(kubectl -n rook-ceph get svc -l app=rook-ceph-mon -o jsonpath='{.items[0].spec.clusterIP}'):3300 2&gt;/dev/null
</code></pre>
<pre><code class="language-bash">ceph v2
</code></pre>
<p>만약 &quot;ceph v2&quot; 가 콘솔에 나타나면, 연결이 성공적인 것입니다. 응답이 없다면, 연결이 정상적으로 이루어지지 않는 것입니다.</p>
<h4 id="mon-pod-비정상"><a class="header" href="#mon-pod-비정상">mon pod 비정상</a></h4>
<p>다음으로 mon pod 이 정상적으로 시작되었는 지 확인할 수 있습니다.</p>
<pre><code class="language-bash">kubectl -n rook-ceph get pod -l app=rook-ceph-mon
</code></pre>
<pre><code class="language-bash">NAME                                READY     STATUS               RESTARTS   AGE
rook-ceph-mon-a-69fb9c78cd-58szd    1/1       CrashLoopBackOff     2          47s
</code></pre>
<p>mon pod 이 정상적으로 시작되지 않았다면, 원인 파악을 위해서 mon pod 의 상태나 로그를 확인해야 합니다. pod 이 crash loop backoff 상태라면, pod 을 describe 하여 원인을 확인해볼 수 있습니다.</p>
<pre><code class="language-bash"># 이미 배포된 키링과 일치하지 않아 pod 이 중단된 경우
kubectl -n rook-ceph describe pod -l mon=rook-ceph-mon0
</code></pre>
<pre><code class="language-bash">...
   Last State:    Terminated
     Reason:    Error
     Message:    The keyring does not match the existing keyring in /var/lib/rook/rook-ceph-mon0/data/keyring.
                   You may need to delete the contents of dataDirHostPath on the host from a previous deployment.
...
</code></pre>
<p>다음 섹션에서 노드의 <code>dataDirHostPath</code> 클린업 방법을 확인하세요</p>
<h4 id="해결방법-2"><a class="header" href="#해결방법-2">해결방법</a></h4>
<p>이 증상은 호스트의 로컬 디렉토리가 정상적으로 삭제되지 않았을 때 다시 Rook 클러스터를 재배포하면 발생합니다. 이 디렉토리는 클러스터 CRD 의 <code>dataDirHostPath</code> 이며, <code>/var/lib/rook</code> 으로 기본 설정되어 있습니다. 이 이슈를 수정하기 위해서는, Rook 의 모든 컴포넌트를 삭제하고 모든 노드에서 <code>/var/lib/rook</code> 디렉토리의 모든 파일들을 삭제합니다. 그러고 나서 새로운 클러스터 CRD 를 배포하게 되면, rook operator 는 모든 pod 들을 예상한대로 배포할 것입니다.</p>
<blockquote>
<p><strong>중요: <code>dataDirHostPath</code> 폴더의 모든 파일을 삭제하면 스토리지 클러스터가 모두 삭제됩니다.</strong></p>
</blockquote>
<p>더 많은 정보는 <a href="ceph_tools//ceph_storage/cleanup.html">클린업 가이드</a> 를 참고합니다.</p>
<h2 id="pvc-가-pending-상태로-지속됨"><a class="header" href="#pvc-가-pending-상태로-지속됨">PVC 가 Pending 상태로 지속됨</a></h2>
<h3 id="증상-2"><a class="header" href="#증상-2">증상</a></h3>
<ul>
<li>Rook 스토리지 클래스로 PVC 를 생성했을떄, Pending 상태로 계속 남아있음</li>
</ul>
<p>Wordpress 예제에서, PVC 가 Pending 상태로 남아있던 것을 이미 보았을 것입니다.</p>
<pre><code class="language-bash">kubectl get pvc
</code></pre>
<pre><code class="language-bash">NAME             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE
mysql-pv-claim   Pending                                      rook-ceph-block   8s
wp-pv-claim      Pending                                      rook-ceph-block   16s
</code></pre>
<h3 id="확인-2"><a class="header" href="#확인-2">확인</a></h3>
<p>PVC 가 Pending 상태로 남는 주요 원인은 아래 두가지입니다.</p>
<ul>
<li>클러스터에 OSD 가 없음</li>
<li>CSI 프로비저너 Pod 이 동작하지 않고 있거나, 스토리지를 프로비저닝하도록 요청을 받고 응답하지 않음</li>
</ul>
<h4 id="osd-가-있는지-확인"><a class="header" href="#osd-가-있는지-확인">OSD 가 있는지 확인</a></h4>
<p>클러스터에 OSD 가 있는지 확인하기 위해서, <a href="ceph_tools//ceph_tools/toolbox.html">Rook Toolbox</a> 에 연결하여 <code>ceph status</code> 커맨드를 입력합니다. 적어도 하나 이상의 OSD 가 <code>up</code> 그리고 <code>in</code> 상태로 존재해야 합니다. 최소 OSD 갯수는 스토리지 클래스를 위해 만들어진 pool 의 <code>replicated.size</code> 설정에 따라 다릅니다. &quot;test&quot; 클러스터에서는, 하나의 OSD 만 필요합니다(<code>storageclass-test.yaml</code> 참고). 프로덕선 스토리지 클래스에서는 (<code>storageclass.yaml</code>) 세개의 OSD 가 필요합니다.</p>
<pre><code class="language-bash">ceph status
</code></pre>
<pre><code class="language-bash"> cluster:
   id:     a0452c76-30d9-4c1a-a948-5d8405f19a7c
   health: HEALTH_OK

 services:
   mon: 3 daemons, quorum a,b,c (age 11m)
   mgr: a(active, since 10m)
   osd: 1 osds: 1 up (since 46s), 1 in (since 109m)
</code></pre>
<h4 id="osd-준비-로그"><a class="header" href="#osd-준비-로그">OSD 준비 로그</a></h4>
<p>예상한 갯수의 OSD 가 보이지 않는다면, 왜 OSD 가 생성되지 못했는지 확인해 봅니다. Rook 이 OSD 를 설정하는 각 노드에서, &quot;osd prepare&quot; pod 이 생성되는 것을 확인했을 것입니다.</p>
<pre><code class="language-bash">kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare
</code></pre>
<pre><code class="language-bash">NAME                                 ...  READY   STATUS      RESTARTS   AGE
rook-ceph-osd-prepare-minikube-9twvk   0/2     Completed   0          30m
</code></pre>
<p><a href="ceph_tools/common_issues.html#osd-pod-%EB%93%A4%EC%9D%B4-%EC%8B%9C%EC%9E%91%EB%90%98%EC%A7%80-%EC%95%8A%EC%9D%8C">osd pod 들이 시작되지 않음</a> 섹션을 참고합니다.</p>
<h4 id="csi-드라이버"><a class="header" href="#csi-드라이버">CSI 드라이버</a></h4>
<p>CSI 드라이버가 요청에 응답하지 못했을 수 있습니다. 프로비저닝 하는 동안 CSI 프로비저너 Pod 에 어떤 에러 로그가 있었는지 확인합니다.</p>
<p>두개의 프로비저너 Pod 이 존재할 것입니다:</p>
<pre><code class="language-bash">kubectl -n rook-ceph get pod -l app=csi-rbdplugin-provisioner
</code></pre>
<p>각 Pod 들의 로그를 확인하세요, 이중 하나가 리더이고, 실제로 요청에 대한 응답을 수행합니다.</p>
<pre><code class="language-bash">kubectl -n rook-ceph logs csi-cephfsplugin-provisioner-d77bb49c6-q9hwq csi-provisioner
</code></pre>
<p>더 많은 이슈는 <a href="ceph_tools//ceph_tools/csi_common_issues.html">CSI 트러블슈팅 가이드</a> 을 참고하세요.</p>
<h4 id="오퍼레이터가-응답하지-않음"><a class="header" href="#오퍼레이터가-응답하지-않음">오퍼레이터가 응답하지 않음</a></h4>
<p>OSD 가 <code>up</code>, <code>in</code> 상태인 것을 확인했다면, 다음 절차는 오퍼레이터가 응답하는지 확인하는 것입니다. PVC 가 생성되고 난 후에 오퍼레이터 pod 에 로그가 발생하는지 확인합니다. 오퍼레이터가 블록 이미지를 프로비저닝 하는 데 요청을 보여주지 않는다면, 다른 작업 때문에 stuck 되었을 수 있습니다. 이럴때는, 오퍼레이터 pod 을 재시작 해줍니다.</p>
<h3 id="해경방법"><a class="header" href="#해경방법">해경방법</a></h3>
<p>&quot;osd prepare&quot; 로그가 왜 OSD 가 생성되지 않았는지에 대한 실마리를 주지 않는다면, <code>cluster.yaml</code> 설정을 확인하세요. 주된 설정 실수는 아래와 같습니다:</p>
<ul>
<li><code>useAllDevices: true</code> 설정이 되어 있으면, Rook 은 노드의 로컬 디바이스를 찾습니다. 디바이스가 노드에 없으면, OSD 는 생성되지 않습니다.</li>
<li><code>useAllDevices: fales</code> 설정이 되어 있으면, OSD 는 <code>deviceFilter</code> 가 명시되어 있어야 생성됩니다.</li>
<li>노드의 로컬 디바이스만 Rook 에 의해 설정됩니다. 다시말해, <code>/dev</code> 경로 아래에 디바이스가 보여야 합니다.
<ul>
<li>디바이스는 파티션이나 파일시스템을 가지고 있어서는 안됩니다. Rook 은 raw 디바이스만 설정합니다. 파티션은 아직 지원되지 않습니다.</li>
</ul>
</li>
</ul>
<h2 id="osd-pod-들이-시작되지-않음"><a class="header" href="#osd-pod-들이-시작되지-않음">OSD Pod 들이 시작되지 않음</a></h2>
<h3 id="증상-3"><a class="header" href="#증상-3">증상</a></h3>
<ul>
<li>OSD 이 시작에 실패함</li>
<li>클러스터를 클린업 한 후 다시 클러스터를 시작함</li>
</ul>
<h3 id="확인-3"><a class="header" href="#확인-3">확인</a></h3>
<p>OSD 가 시작할 때, 디바이스나 디렉토리가 설정됩니다. 설정에 에러가 있으면, pod 은 CrashLoopBackoff 상태로 시작되지 않습니다. 실패 원인을 조회하려면 osd pod 로그를 확인합니다.</p>
<pre><code class="language-bash">$ kubectl -n rook-ceph logs rook-ceph-osd-fl8fs
...
</code></pre>
<p>주된 실패의 원인은 테스트 클러스터를 다시 배포했을 때, 기존 배포 떄의 상태가 남아있어 생깁니다. 만약 클러스터의 노드가 많다면, monitor 들이 쿼럼을 형성해 따시 시작할 수 있을 수도 있습니다. 하지만, osd pod 들은 기존 상태 때문에 시작하지 못할 것입니다. OSD pod 로그를 확인한다면, 그에 대한 로그를 확인할 수 있을 것입니다.</p>
<pre><code class="language-bash">$ kubectl -n rook-ceph logs rook-ceph-osd-fl8fs
</code></pre>
<pre><code class="language-bash">...
2017-10-31 20:13:11.187106 I | mkfs-osd0: 2017-10-31 20:13:11.186992 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _read_fsid unparsable uuid
2017-10-31 20:13:11.187208 I | mkfs-osd0: 2017-10-31 20:13:11.187026 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _setup_block_symlink_or_file failed to create block symlink to /dev/disk/by-partuuid/651153ba-2dfc-4231-ba06-94759e5ba273: (17) File exists
2017-10-31 20:13:11.187233 I | mkfs-osd0: 2017-10-31 20:13:11.187038 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) mkfs failed, (17) File exists
2017-10-31 20:13:11.187254 I | mkfs-osd0: 2017-10-31 20:13:11.187042 7f0059d62e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (17) File exists
2017-10-31 20:13:11.187275 I | mkfs-osd0: 2017-10-31 20:13:11.187121 7f0059d62e00 -1  ** ERROR: error creating empty object store in /var/lib/rook/osd0: (17) File exists
</code></pre>
<h3 id="해결방법-3"><a class="header" href="#해결방법-3">해결방법</a></h3>
<p>기존 설정이 존재해서 에러가 나타났다면, 기존에 사용하던 로컬 디렉토리가 제대로 삭제되지 않은 것입니다. 이 디렉토리는 <code>dataDirHostPath</code> 설정을 통해 설정되는 디렉토리이며, 기본값은 <code>/var/lib/rook</code> 입니다. 이 이슈를 해결하기 위해서는, 모든 호스트에서 <code>/var/lib/rook</code> 디렉토리의 파일들을 삭제해야 합니다. (혹은 <code>dataDirHostPath</code> 에 설정된 디렉토리) 이후 신규 클러스터갑 배포된다면, rook 오퍼레이터는 모든 pod 들을 예상한대로 실행할 것입니다.</p>
<h2 id="osd-pod-들이-디바이스에-생성되지-않음"><a class="header" href="#osd-pod-들이-디바이스에-생성되지-않음">OSD Pod 들이 디바이스에 생성되지 않음</a></h2>
<h3 id="증상-4"><a class="header" href="#증상-4">증상</a></h3>
<ul>
<li>클러스터에서 OSD pod 이 시작되지 않음</li>
<li>클러스터 CRD 에 명시 되었으나 OSD 와 함께 디바이스가 설정되지 않음</li>
<li>개별 디바이스별로 여러 Pod 이 생성되지 않고 각 노드별로 하나의 OSD 만 생성됨 </li>
</ul>
<h3 id="확인-4"><a class="header" href="#확인-4">확인</a></h3>
<p>먼저, CRD 에 디바이스가 올바르게 지정되어 있는지 확인합니다. <a href="ceph_tools//ceph_storage/cluster_crd.html">Cluster CRD</a> 는 Rook 에 의해 디바이스가 어떻게 사용될 것인지 명시하는 여러 방법을 제공합니다.</p>
<ul>
<li><code>useAllDevices: true</code>: 가능한 모든 디바이스를 사용합니다.</li>
<li><code>deviceFilter</code>: 정규 표현식에 매치되는 모든 디바이스를 사용합니다.</li>
<li><code>devices</code>: 각 노드별로 사용할 디바이스를 명시적으로 지정합니다.</li>
</ul>
<p>두번째로, Rook 이 가용한 디바이스가 없다고 판단하면, (이미 파티션이나 파일시스템이 있음), Rook 은 해당 디바이스를 사용하지 않습니다. OSD 가 시작되지 않으면, 이 이슈일 가능성이 있습니다. 디바이스가 스킵되었는지 확인하려면, OSD prepare 로그를 확인합니다. osd prepare pod 이 <code>completed</code> 상태가 된 것은 정상적인 상태입니다. job 이 완료되고 나면, 로그를 확인해야 할 케이스를 대비하여 pod 을 남겨놓습니다.</p>
<pre><code class="language-bash"># 클러스터 안에서 osd prepare pod 조회
kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare
</code></pre>
<pre><code class="language-bash">NAME                                   READY     STATUS      RESTARTS   AGE
rook-ceph-osd-prepare-node1-fvmrp      0/1       Completed   0          18m
rook-ceph-osd-prepare-node2-w9xv9      0/1       Completed   0          22m
rook-ceph-osd-prepare-node3-7rgnv      0/1       Completed   0          22m
</code></pre>
<pre><code class="language-bash"># &quot;provision&quot; 컨테이너에서 노드의 로그를 조회합니다.
kubectl -n rook-ceph logs rook-ceph-osd-prepare-node1-fvmrp provision
[...]
</code></pre>
<p>로그에서 유용한 라인들:</p>
<pre><code class="language-bash"># Rook 이 파티션이나 파일시스템을 발견하면 디바이스는 스킵됩니다.
2019-05-30 19:02:57.353171 W | cephosd: skipping device sda that is in use
2019-05-30 19:02:57.452168 W | skipping device &quot;sdb5&quot;: [&quot;Used by ceph-disk&quot;]

# ceph 이 사용할 수 없는 디스크에 대한 메시지:
Insufficient space (&lt;5GB) on vgs
Insufficient space (&lt;5GB)
LVM detected
Has BlueStore device label
locked
read-only

# 디바이스가 설정됨
2019-05-30 19:02:57.535598 I | cephosd: device sdc to be configured by ceph-volume

# 각 디바이스마다 로그에 리포트가 프린트됩니다.
2019-05-30 19:02:59.844642 I |   Type            Path                                                    LV Size         % of device
2019-05-30 19:02:59.844651 I | ----------------------------------------------------------------------------------------------------
2019-05-30 19:02:59.844677 I |   [data]          /dev/sdc                                                7.00 GB         100%
</code></pre>
<h3 id="해결방법-4"><a class="header" href="#해결방법-4">해결방법</a></h3>
<p>커스텀 리소스를 올바른 방법으로 수정하거나, 디바이스에서 파티션이나 파일시스템을 클린업합니다. 이전 설치로부터 디바이스를 클린업하려면 <a href="ceph_tools//ceph_storage/cleanup.html#zapping-devices">가이드</a>를 참고하세요.</p>
<p>설정이 업데이트되거나 디바이스가 클린업 되고 나면, 오퍼레이터를 재시작함으로서 디바이스를 다시 분석하도록 트리거합니다. 오퍼레이터가 재시작 될때마다, 원하는 상태로 디바이스가 설정되었는지 확인합니다. 대부분의 시나리오에서 오퍼레이터는 OSD 를 자동으로 배포하지만, 오퍼레이터가 자동으로 감지하지 못하는 시나리오에 대해서는 재시작 시 대부분 해결됩니다.</p>
<pre><code class="language-bash"># 디바이스가 설정되었는지 확인하기 위해서 오퍼레이터를 재시작합니다. 새로운 pod 은 자동으로 다시 시작됩니다.
kubectl -n rook-ceph delete pod -l app=rook-ceph-operator
[...]
</code></pre>
<h2 id="노드-리부팅-이후-노드가-hang-상태로-지속됨"><a class="header" href="#노드-리부팅-이후-노드가-hang-상태로-지속됨">노드 리부팅 이후 노드가 Hang 상태로 지속됨</a></h2>
<p>이 이슈는 Rook v1.3 이후 버전에서 수정되었습니다.</p>
<h3 id="증상-5"><a class="header" href="#증상-5">증상</a></h3>
<ul>
<li><code>reboot</code> 명령 이후에 노드가 온라인 상태로 돌아오지 않음</li>
<li>전원 버튼만이 해결책</li>
</ul>
<h3 id="확인-5"><a class="header" href="#확인-5">확인</a></h3>
<p>Ceph persistent volume 을 사용중인 pod 의 노드에서 아래 명령을 입력합니다.</p>
<pre><code class="language-bash">mount | grep rbd
</code></pre>
<pre><code class="language-bash"># _netdev 마운트 옵션이 없고, cephfs 도 동일
# OS 는 PV 가 네트워크를 통해 마운트 되었는지 알지 못합니다.
/dev/rbdx on ... (rw,relatime, ..., noquota)
</code></pre>
<p>리부팅 커맨드가 실행되면, 네트워크 인터페이스가 디스크 unmount 전에 종료됩니다. 이 때문에 Ceph persistent volume 의 umount 는 계속 실패하며 아래의 에러와 함께 계속 재시도합니다.</p>
<pre><code class="language-bash">libceph: connect [monitor-ip]:6789 error -101
</code></pre>
<h3 id="해결방법-5"><a class="header" href="#해결방법-5">해결방법</a></h3>
<p>노드가 리부팅되기 전에 <a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">drain</a> 되어야 합니다. drain 이 완료된 이후에야 노드가 리부팅 될 수 있습니다.</p>
<p><code>kubectl drain</code> 커맨드는 자동으로 노드를 스케줄링 되지 않게 하기 때문에 (<code>kubectl cordon</code>), 온라인으로 돌아오고 난 이후에는 uncordon 되어야 합니다.</p>
<h4 id="노드-drain"><a class="header" href="#노드-drain">노드 Drain:</a></h4>
<pre><code class="language-bash">$ kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-local-data
</code></pre>
<h4 id="노드-uncordon"><a class="header" href="#노드-uncordon">노드 Uncordon</a></h4>
<pre><code class="language-bash">$ kubectl uncordon &lt;node-name&gt;
</code></pre>
<h2 id="여러-cephfs-를-커널-47-미만-버전에서-사용하려고-함"><a class="header" href="#여러-cephfs-를-커널-47-미만-버전에서-사용하려고-함">여러 Cephfs 를 커널 4.7 미만 버전에서 사용하려고 함</a></h2>
<h3 id="증상-6"><a class="header" href="#증상-6">증상</a></h3>
<ul>
<li>하나 이상의 공유 파일시스템 (Cephfs) 를 클러스터에 만듬</li>
<li>pod 이 <strong>처음</strong> 생성된 공유 파일시스템 외에 다른 공유 파일시스템을 마운트하려 합니다.</li>
<li>pod 이 의도한 파일시스템 대신 마운트된 첫 번째 파일시스템을 잘못 가져옵니다.</li>
</ul>
<h3 id="해결방법-6"><a class="header" href="#해결방법-6">해결방법</a></h3>
<p>이 문제를 해결할 유일한 방법은 커널을 <code>4.7</code> 이상으로 업그레이드 하는 것입니다. 이름으로 파일시스템을 선택하는 마운트 플래그가 <code>4.7</code> 에서 추가되었습니다.</p>
<p>다중 공유 파일시스템에 대한 더 자세한 커널 버전 요구사항은 <a href="ceph_tools//ceph_storage/shared_filesystem.html#kernel-version-requirement">파일시스템 - 커널 버전 요구사항</a> 섹션을 참고하세요.</p>
<h2 id="모든-ceph-데몬들의-로그-레벨을-debug-로-변경"><a class="header" href="#모든-ceph-데몬들의-로그-레벨을-debug-로-변경">모든 Ceph 데몬들의 로그 레벨을 Debug 로 변경</a></h2>
<p>모든 Ceph 데몬에 대해 동시에 로그 레벨을 변경할 수 있습니다. 이를 위해서, toolbox pod 이 동작 중인지 확인하고, 0-20 까지 원하는 로그 레벨을 정합니다. 모든 서비시스템의 로그레벨과 기본값을 <a href="https://docs.ceph.com/en/latest/rados/troubleshooting/log-and-debug/#ceph-subsystems">다음</a> 가이드에서 확인 가능합니다. 로그 레벨을 높일수록 더 많은 로그를 발생시킵니다.</p>
<p>로그 레벨 1을 원하면, 아래 커맨드를 입력합니다.</p>
<pre><code class="language-bash">kubectl -n rook-ceph exec deploy/rook-ceph-tools -- set-ceph-debug-level 1
</code></pre>
<p>결과:</p>
<pre><code class="language-bash">ceph config set global debug_context 1
ceph config set global debug_lockdep 1
...
...
</code></pre>
<p>디버깅이 완료되었다면, 아래 커맨드로 기본값으로 돌릴 수 있습니다.</p>
<pre><code class="language-bash">kubectl -n rook-ceph exec deploy/rook-ceph-tools -- set-ceph-debug-level default
</code></pre>
<h2 id="특정-ceph-데몬에-대해-file-로-로그를-쓰도록-변경"><a class="header" href="#특정-ceph-데몬에-대해-file-로-로그를-쓰도록-변경">특정 Ceph 데몬에 대해 file 로 로그를 쓰도록 변경</a></h2>
<p>쿠버네티스 로그만으로는 다양한 원인을 파악하기에는 한계가 있을 수 있습니다.</p>
<ul>
<li>모든 사람들에게 쿠버네티스의 로깅 방법이 로그를 찾는 데 적절하지는 않으며, 전통적인 디렉토리 내에 로그를 남기는 방법이 편리할 수 있습니다.</li>
<li>로그 엔진의 버퍼를 넘어서면 로그는 로테이트 되어 사라집니다.</li>
</ul>
<p>그래서 각각의 데몬에서, 로깅이 활성화 되어 있다면 <code>dataDirHostPath</code> 설정이 로그를 저장하기 위해서 사용됩니다. Rook 는 <code>dataDirHostPath</code> 를 각 pod 마다 bind mount 합니다. 여러분이 <code>mon.a</code> 데몬만 로깅을 활성화 한다고 가정한다면, toolbox 내에서 아래 커맨드를 사용합니다. </p>
<pre><code class="language-bash">ceph config set mon.a log_to_file true
</code></pre>
<p>이 커맨드는 파일시스템에 로깅하도록 활성화시키며, 로그를 <code>${dataDirHostPath}/$NAMESPACE/log</code> 디렉토리에서 확인할 수 있습니다. 기본 설정을 사용한다면, <code>/var/lib/rook/rook-ceph/log</code> 에서 확인 가능합니다. pod 을 재시작하지 않아도 즉시 반영됩니다.</p>
<p>파일로의 로깅을 비활성화하려면, <code>log_to_file</code> 을 <code>false</code> 로 설정합니다.</p>
<h2 id="rbd-디바이스를-사용하는-워커-노드가-hang-상태로-지속됨"><a class="header" href="#rbd-디바이스를-사용하는-워커-노드가-hang-상태로-지속됨">RBD 디바이스를 사용하는 워커 노드가 Hang 상태로 지속됨</a></h2>
<h3 id="증상-7"><a class="header" href="#증상-7">증상</a></h3>
<ul>
<li>RBD 디바이스 중 하나에 I/O 가 없음 (<code>/dev/rbd*</code> 혹은 <code>/dev/nbd*</code>)</li>
<li>모든 워커 노드가 hang 됨</li>
</ul>
<h3 id="확인-6"><a class="header" href="#확인-6">확인</a></h3>
<p>위 현상은 아래와 같은 상황에서 발생합니다.</p>
<ul>
<li>문제가 있는 RBD 디바이스와 OSD 가 동시에 있을 떄 </li>
<li>디바이스가 XFS 파일시스템일 떄 </li>
</ul>
<p>문제가 발생했을 때, <code>dmesg</code> 커맨드로 로그를 확인할 수 있습니다.</p>
<pre><code class="language-bash">dmesg
</code></pre>
<pre><code class="language-bash">...
[51717.039319] INFO: task kworker/2:1:5938 blocked for more than 120 seconds.
[51717.039361]       Not tainted 4.15.0-72-generic #81-Ubuntu
[51717.039388] &quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&quot; disables this message.
...
</code></pre>
<p>이 문제는 <code>hung_task</code> 문제라고도 불리며, 커널 내에 데드락이 발생했다는 것을 의미합니다. 더 많은 정보는, <a href="https://github.com/rook/rook/issues/3132#issuecomment-580508760">해당 이슈 코멘트</a> 를 확인하세요.</p>
<h3 id="해결방법-7"><a class="header" href="#해결방법-7">해결방법</a></h3>
<p>아래 두 방법을 통해 문제를 해결할 수 있습니다.</p>
<ul>
<li>Linux Kernel: <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=8d19f1c8e1937baf74e1962aae9f90fa3aeab463">다음 커밋</a> 에서 해당 마이너 피처가 소개되었고, 커널 v5.6 에 포함되었습니다.</li>
<li>Ceph: 커널 v5.6 릴리즈 이후에 위에서 언급된 커널 피처를 사용하는 수정사항에 대한 논의가 이루어질 것입니다.</li>
</ul>
<p>XFS 파일시스템 대신 ext4 파일시스템을 사용하여 위 문제를 회피할 수 있습니다. 스토리지 클래스 리소스에 <code>csi.storage.k8s.io/fstype</code> 에 파일시스템을 명시할 수 있습니다.</p>
<h2 id="too-few-pgs-per-osd-메시지가-나타남"><a class="header" href="#too-few-pgs-per-osd-메시지가-나타남">Too few PGs per OSD 메시지가 나타남</a></h2>
<h3 id="증상-8"><a class="header" href="#증상-8">증상</a></h3>
<ul>
<li><code>ceph status</code> 커맨드를 입력했을 때, &quot;too few PGs per OSD&quot; 경고 메시지가 나타남</li>
</ul>
<pre><code class="language-bash">ceph status
</code></pre>
<pre><code class="language-bash"> cluster:
   id:     fd06d7c3-5c5c-45ca-bdea-1cf26b783065
   health: HEALTH_WARN
           too few PGs per OSD (16 &lt; min 30)
</code></pre>
<h3 id="해결방법-8"><a class="header" href="#해결방법-8">해결방법</a></h3>
<p><a href="https://docs.ceph.com/docs/master/rados/operations/health-checks#too-few-pgs">이 문서</a> 에서 경고의 의미를 조회할 수 있습니다. 더 많은 정보는 <a href="https://ceph.com/community/new-luminous-pg-overdose-protection/">이 블로그</a> 를 참고하세요. pool 의 적절한 <code>pg_num</code> 을 알고 싶거나 이 값을 변경하고 싶으면 <a href="ceph_tools//ceph_tools/advanced_configuration.html#configuring-pools">Configuring Pools</a> 섹션을 참고하세요.</p>
<h2 id="lv-백엔드-pvc-를-사용하는-osd-에서-lvm-메타데이터가-손상됨"><a class="header" href="#lv-백엔드-pvc-를-사용하는-osd-에서-lvm-메타데이터가-손상됨">LV 백엔드 PVC 를 사용하는 OSD 에서 LVM 메타데이터가 손상됨</a></h2>
<h3 id="증상-9"><a class="header" href="#증상-9">증상</a></h3>
<p>LV 기반 PVC 에는 OSD 에 심각한 결함이 있습니다. 호스트와 OSD 컨테이너에서 동시에 LVM 메타데이터를 수정하면 메타데이터가 꺠질 수 있습니다. 예를 들어, 운영자가 호스트에서 LVM 메타데이터를 수정하는 와중에, OSD 초기화 프로세스가 컨테이너 내에서 일어날 수 있습니다. 또한, <code>lvmetad</code> 가 동작 중이라면, 더 자주 발생할 수 있습니다. 이 경우, OSD 컨테이너의 LVM 메타데이터 변경은 한동안 호스트의 LVM 메타데이터 캐시에 반영되지 않습니다.</p>
<p>LVM 위에서 OSD 를 동작시키기로 결정했다면, 이슈 발생 가능성을 줄이기 위해 아래 사항들을 유의해주세요.</p>
<h3 id="해결방법-9"><a class="header" href="#해결방법-9">해결방법</a></h3>
<ul>
<li><code>lvmetad</code> 를 비활성화합니다.</li>
<li>호스트로부터 LV 를 설정하지 않습니다. 그리고, LV 뒷단의 VG 들과 물리적 볼륨들을 수정하지 않습니다.</li>
<li><code>storageClassDeviceSets</code> 의 <code>count</code> 필드가 증가하지 않도록 하고, OSD 를 동시에 지원하는 새 LV 를 생성합니다.</li>
</ul>
<p><code>sudo lvs -o lv_name,lv_tags</code> 명령을 사용해서 위의 태그가 존재하는지 여부를 알 수 있습니다. 만약 OSD lv_tags 에 해당하는 LV 에서 <code>lv_tag</code> 필드가 비어있다면 이 OSD 에 문제가 발생한 것입니다. 이 경우, OSD 를 시작하기 전에 이 <a href="ceph_tools//osd_management.html#remove-an-osd">OSD 를 제거</a> 하거나 교체하세요.</p>
<p>이 문제는 OSD 컨테이너가 LVM 메타데이터를 더이상 건드리지 않기 떄문에 신규로 생성된 LV 백엔드 PVC 에서는 발생하지 않습니다. 이미 lvm 모드로 만들어진 OSD 는 Rook 가 업그레이드 되더라도 계속 동작합니다. 그러나, 위의 이슈들 떄문에 raw 모드 OSD 가 권장됩니다. 이미 존재하는 OSD 들을 raw 모드로 변경할 수 있습니다. <a href="ceph_tools//osd_management.html#remove-an-osd">OSD 제거</a> 와 <a href="ceph_tools//osd_management.html#add-an-osd-on-a-pvc">OSD 추가</a> 문서를 확인하세요</p>
<h2 id="osd-prepare-job-이-low-aio-max-nr-설정으로-실패함"><a class="header" href="#osd-prepare-job-이-low-aio-max-nr-설정으로-실패함">OSD Prepare Job 이 low aio-max-nr 설정으로 실패함</a></h2>
<p>만약 커널이 low <a href="https://www.kernel.org/doc/Documentation/sysctl/fs.txt">aio-max-nr 설정</a> 으로 구성되어 있다면, OSD prepare job 이 아래 에러와 함께 실패할 것입니다.</p>
<pre><code class="language-bash">exec: stderr: 2020-09-17T00:30:12.145+0000 7f0c17632f40 -1 bdev(0x56212de88700 /var/lib/ceph/osd/ceph-0//block) _aio_start io_setup(2) failed with EAGAIN; try increasing /proc/sys/fs/aio-max-nr
</code></pre>
<p>이 에러를 해결하기 위해서는, sysctl 설정에서 <code>fs.aio-max-nr</code> 값을 늘려 주어야 합니다. (<code>/etc/sysctl.conf</code>) 여러분이 가장 선호하는 설정 관리 시스템으로 변경할 수 있습니다.</p>
<p>대안으로, <a href="https://github.com/rook/rook/issues/6279#issuecomment-694390514">DaemonSet</a> 을 사용하여 모든 노드에 설정을 적용할 수 있습니다.</p>
<h2 id="예상하지-않은-파티션이-생성됨"><a class="header" href="#예상하지-않은-파티션이-생성됨">예상하지 않은 파티션이 생성됨</a></h2>
<h3 id="증상-10"><a class="header" href="#증상-10">증상</a></h3>
<p><strong>Rook 버전 v1.6.0-v1.6.7 을 사용하는 사용자들은 예기치 않게 무작위로 나타나는 파티션에 원하지 않는 OSD 를 확인할 수도 있으며, 이로 인해 기존의 OSD 가 손상될 수 있습니다.</strong></p>
<p>Ceph OSD 에 의해 사용되는 호스트의 디스크에 예기치 못한 파티션이 만들어 질 수 있습니다. SSD 보다 HDD 에 더 많이 나타나며, 875GB 보다 높은 디스크에서 더 발생합니다. <code>lsblk</code>, <code>blkid</code>, <code>udevadm</code>, <code>parted</code> 같은 툴들은 이런 파티션에 대해 파티션 타입을 보여주지 않을 수 있습니다. <code>blkid</code> 신규 버전은 타입을 &quot;atari&quot; 로 인식합니다.</p>
<p>이런 이슈는 리눅스 커널의 Atari 파티션 (AHDI 로 구분되는) 지원입니다. Atari 파티션은 다른 파티션 유형에 비해 매우 완화된 사양을 가지고 있으며, 디스크에 작성된 임의의 데이터가 리눅스 커널의 Atari 파티션으로 표시될 수 있습니다. Ceph 의 블루스토어 OSD 는 커널에 Atari 파티션으로 나타날 수 있는 디스크에 데이터를 쓸 확률이 높습니다.</p>
<p>아래는 Atari 파티션이 나타난 특정 노드의 <code>lsblk</code> 아웃풋을 나타냅니다. <code>sdX1</code> 은 팬텀 파티션으로 나타나지 않으며, 모든 디스크에는 <code>sdX2</code> 가 48GB 로 나타납니다. <code>sdX3</code> 는 가변 크기이며 항상 존재하지는 않을 수 있습니다. 드물지만 <code>sdX4</code> 가 나타날 수 있습니다.</p>
<pre><code class="language-bash"># lsblk
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sdb      8:16   0     3T  0 disk
├─sdb2   8:18   0    48G  0 part
└─sdb3   8:19   0   6.1M  0 part
sdc      8:32   0     3T  0 disk
├─sdc2   8:34   0    48G  0 part
└─sdc3   8:35   0   6.2M  0 part
sdd      8:48   0     3T  0 disk
├─sdd2   8:50   0    48G  0 part
└─sdd3   8:51   0   6.3M  0 part
</code></pre>
<p>더 많은 정보는 https://github.com/rook/rook/issues/7940 에서 확인할 수 있습니다. </p>
<h3 id="해결방법-10"><a class="header" href="#해결방법-10">해결방법</a></h3>
<h4 id="corruption-으로부터-복구하기"><a class="header" href="#corruption-으로부터-복구하기">corruption 으로부터 복구하기</a></h4>
<p>여러분이 Rook v1.6 을 사용중이라면, Atari 파티션으로 인해 발생하는 OSD corruption 사고를 피하기 위해 v1.6.8 이상 버전을 사용해야 합니다.</p>
<p>기존에는 <code>deviceFilter: ^sd[a-z]+$</code> 를 사용하는 것으로 해결방안을 제시했지만, 그래도 예기치 못한 파티션이 발생합니다. Rook 은 새 OSD 를 만드는 것을 멈출 뿐이며, Atari 파티션 문제를 인식하지 못하는 <code>ceph-volume</code> 과 관련된 문제는 해결되지 않았습니다. 이 해결 방법을 사용한 사용자는 앞으로도 OSD 오류가 발생할 위험이 있습니다.</p>
<p>이 이슈를 해결하기 위해서, 즉시 v1.6.8 로 업데이트를 해야 합니다. 업데이트 이후, 미래에 생성되는 OSD 들에 대해서는 corruption 이 발생해선 안됩니다. 이후 클러스터가 Healty 상태로 돌아가면, 이미 손상된 디스크에 대해서 하나씩 <a href="ceph_tools//osd_management.html#remove-an-osd">OSD 제거</a> 를 진행합니다.</p>
<p>예시와 같이, 여러분은 두개의 예상치 못한 파티션(<code>/dev/sdb2</code>, <code>/dev/sdb3</code>)을 가진 <code>/dev/sdb</code> 디바이스를 가지고 있고, 두번째 손상 디스크 <code>/dev/sde</code> 는 하나의 예상하지 못한 파티션 (<code>/dev/sde2</code>) 를 가지고 있습니다.</p>
<ol>
<li>먼저, <code>/dev/sdb</code>, <code>/dev/sdb2</code>, <code>/dev/sdb3</code> 와 관련된 OSD 를 삭제합니다. OSD 하나일 수 도 있고, 세 개의 OSD 가 연관이 있을 수 도 있습니다. <a href="ceph_tools//osd_management.html">OSD Management</a> 섹션을 참고합니다.</li>
<li>디스크 파티션의 첫번째 섹터를 삭제하기 위해 <code>dd</code> 명령을 사용합니다.
<ul>
<li><code>dd if=/dev/zero of=/dev/sdb2 bs=1M</code></li>
<li><code>dd if=/dev/zero of=/dev/sdb3 bs=1M</code></li>
<li><code>dd if=/dev/zero of=/dev/sdb bs=1M</code></li>
</ul>
</li>
<li>그러고 나서, 신규 OSD 를 준비하기 위해 <code>/dev/sdb</code> 를 클린업합니다. <a href="ceph_tools//ceph_storage/cleanup.html#zapping-devices">teardown 문서</a>를 참고하세요.</li>
<li><code>/dev/sdb</code> 디바이스에 신규 OSD 를 배포하기 위해 Rook 오퍼레이터를 스케일업합니다. 이를 통해 Ceph 은 다음 OSD 가 제거되는 동안 데이터 복구 및 복제를 위해 <code>/dev/sdb</code> 를 사용할 수 있습니다.</li>
<li>1-4 스텝을 <code>/dev/sde</code> 와 <code>/dev/sde2</code> 에 대해 반복합니다. </li>
</ol>
<p>Rook-Ceph 이 중요한 데이터를 가지고 있지 않다면, Rook 을 완전히 삭제하고 v1.6.8 이상으로 재설치 하는 것이 더 간단할 수 있습니다.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="csi-common-issues"><a class="header" href="#csi-common-issues">CSI Common Issues</a></h1>
<p>Ceph CSI 드라이버를 사용해 볼륨을 프로비저닝 할 때, 다음과 같은 여러 이유로 이슈가 발생할 수 있습니다:</p>
<ul>
<li>CSI pod 와 ceph 간의 통신 이슈</li>
<li>클러스터 health 이슈</li>
<li>Slow operation</li>
<li>쿠버네티스 이슈</li>
<li>Ceph-CSI 설정이나 버그</li>
</ul>
<p>아래의 트러블슈팅 스텝이 이슈들을 확인하는 데 도움을 줄 것입니다.</p>
<h3 id="block-rbd"><a class="header" href="#block-rbd">Block (RBD)</a></h3>
<p>블록 볼륨 (RWO) 을 마운트 한다면, Ceph 에서 <code>RBD</code> 볼륨이라고 불립니다. 블록 볼륨에 대한 이슈가 있다면 아래의 RBD 섹션을 참고하세요.</p>
<h3 id="shared-filesystem-cephfs"><a class="header" href="#shared-filesystem-cephfs">Shared Filesystem (Cephfs)</a></h3>
<p>공유 파일시스템 (RWX) 를 마운트 한다면, Ceph 에서는 <code>Cephfs</code> 라고 부릅니다. 공유 파일시스템에 대한 이슈는 CephFS 섹션을 참고하세요.</p>
<h2 id="network-connectivity"><a class="header" href="#network-connectivity">Network Connectivity</a></h2>
<p>mon 은 가장 먼저 확인해야 할 중요한 컴포넌트입니다. Service 리소스로부터 mon 엔드포인트를 확인합니다.</p>
<pre><code class="language-bash">kubectl -n rook-ceph get svc -l app=rook-ceph-mon
</code></pre>
<pre><code class="language-bash">NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
rook-ceph-mon-a   ClusterIP   10.104.165.31   &lt;none&gt;        6789/TCP,3300/TCP   18h
rook-ceph-mon-b   ClusterIP   10.97.244.93    &lt;none&gt;        6789/TCP,3300/TCP   21s
rook-ceph-mon-c   ClusterIP   10.99.248.163   &lt;none&gt;        6789/TCP,3300/TCP   8s
</code></pre>
<p>CephCluster 커스텀 리소스에서 호스트 네트워크가 활성화 되어 있다면, mon 이 동작하고 있는 노드의 IP 를 확인해야 합니다.</p>
<p><code>clusterIP</code> 는 mon IP 이고, <code>3300</code> 은 Ceph-CSI 가 ceph 클러스터에 연결하기 위한 포트입니다. 이 엔드포인트는 CSI 드라이버 뿐만 아니라 모든 클라이언트에서 접근이 가능해야 합니다.</p>
<p>만약 PVC 를 프로비저닝 하는 데 이슈가 발생했다면, 프로비저너 pod 으로부터 네트워크 연결을 확인해야 합니다.</p>
<ul>
<li>CephFS PVC 는, <code>csi-cephfsplugin-provisioner</code> pod 의 <code>csi-cephfsplugin</code> 컨테이너를 확인합니다.</li>
<li>Block PVC 는, <code>csi-rbdplugin-provisioner</code> pod 의 <code>csi-rbdplugin</code> 컨테이너를 확인합니다.</li>
</ul>
<p>이중화를 위해 유형별로 두 개의 프로버지너 Pod 이 있습니다. 모든 Pod 의 연결을 테스트 해야 합니다.</p>
<p>프로비저너 Pod 에 연겨하고, 아래와 같이 mon 엔드포인트로의 연결을 확인합니다.</p>
<pre><code class="language-bash"># Connect to the csi-cephfsplugin container in the provisioner pod
kubectl -n rook-ceph exec -ti deploy/csi-cephfsplugin-provisioner -c csi-cephfsplugin -- bash

# Test the network connection to the mon endpoint
curl 10.104.165.31:3300 2&gt;/dev/null
ceph v2
</code></pre>
<p>&quot;ceph v2&quot; 응답을 확인했다면, 연결은 성공적인 것입니다. 응답이 없다면, ceph 클러스터 연결에 문제가 있는 것입니다.</p>
<p>모든 monitor IP 와 포트에 대해 연결을 확인해야 합니다.</p>
<h2 id="ceph-health"><a class="header" href="#ceph-health">Ceph Health</a></h2>
<p>Ceph 클러스터의 Health 상태가 PVC 를 생성하거나 마운트 하는 데 이슈를 야기할 수 있습니다. <a href="ceph_tools//ceph_tools/toolbox.html">Toolbox</a> 를 통해 Ceph 클러스터에 연결하여 클러스터 상태를 확인합니다.</p>
<pre><code class="language-bash">ceph health detail
</code></pre>
<pre><code class="language-bash">HEALTH_OK
</code></pre>
<h2 id="slow-operation"><a class="header" href="#slow-operation">Slow Operation</a></h2>
<p>클러스터의 slow ops 상태도 이슈를 발생시킬 수 있습니다. toolbox 에서 ceph 클러스터에 slow ops 가 발생하는지 확인합니다.</p>
<pre><code class="language-bash">ceph -s
</code></pre>
<pre><code class="language-bash">cluster:
 id:     ba41ac93-3b55-4f32-9e06-d3d8c6ff7334
 health: HEALTH_WARN
         30 slow ops, oldest one blocked for 10624 sec, mon.a has slow ops
</code></pre>
<p>Ceph 클러스터가 healty 상태가 아니라면, 다음을 확인합니다.</p>
<ul>
<li>Ceph mon 에러 로그</li>
<li>OSD 에러 로그</li>
<li>디스크 상태</li>
<li>네트워크 상태</li>
</ul>
<h2 id="ceph-troubleshooting"><a class="header" href="#ceph-troubleshooting">Ceph Troubleshooting</a></h2>
<h3 id="check-if-the-rbd-pool-exists"><a class="header" href="#check-if-the-rbd-pool-exists">Check if the RBD Pool exists</a></h3>
<p>ceph 클러스터에 <code>storageclass.yaml</code> 에 정의한 pool 이 만들어 졌는지 확인해야 합니다.</p>
<p><code>storageclass.yaml</code> 에 지정한 pool 이름이 <code>replicapool</code> 이라고 합시다. toolbox 에서 아래와 같이 존재 여부를 확인할 수 있습니다.</p>
<pre><code class="language-bash">ceph osd lspools
</code></pre>
<pre><code class="language-bash">1 device_health_metrics
2 replicapool
</code></pre>
<p>pool 이 리스트에 없다면, <code>CephBlockPool</code> 커스텀 리소스를 생성합니다. 이미 pool 을 생성했다면, Rook 오퍼레이터의 에러 로그를 확인합니다.</p>
<h3 id="check-if-the-filesystem-exists"><a class="header" href="#check-if-the-filesystem-exists">Check if the Filesystem exists</a></h3>
<p>공유 파일시스템 (CephFS) 에서, <code>storageclass.yaml</code> 에 정의한 파일시스템과 pool 이 Ceph 클러스터에 존재하는지 확인합니다.</p>
<p><code>storageclass.yaml</code> 에 정의한 <code>fsName</code> 이 <code>myfs</code> 라고 가정해 봅시다. toolbox에서 아래와 같이 확인할 수 있습니다.</p>
<pre><code class="language-bash">ceph fs ls
</code></pre>
<pre><code class="language-bash">name: myfs, metadata pool: myfs-metadata, data pools: [myfs-data0 ]
</code></pre>
<p>다음으로 <code>storageclass.yaml</code> 에 정의된 <code>pool</code> 이 있는지 확인합니다. 여기서는 <code>myfs-data0</code> 입니다.</p>
<pre><code class="language-bash">ceph osd lspools
</code></pre>
<pre><code class="language-bash">1 device_health_metrics
2 replicapool
3 myfs-metadata0
4 myfs-data0
</code></pre>
<p>CephFilesystem 커스텀 리소스로 생성된 pool 은 <code>-data0</code> suffix 를 가집니다.</p>
<h3 id="subvolumegroups"><a class="header" href="#subvolumegroups">subvolumegroups</a></h3>
<p>ceph-csi configmap 에 subvolumegroup 이 명시되어 있지 않다면, (ceph mon 정보를 기입한 곳) Ceph-CSI 는 csi 이름으로 기본 subvolumegroup 을 생성합니다. subvolumegroup 이 존재하는지 확인합니다.</p>
<pre><code class="language-bash">ceph fs subvolumegroup ls myfs
</code></pre>
<pre><code class="language-bash">[
   {
       &quot;name&quot;: &quot;csi&quot;
   }
]
</code></pre>
<p>Ceph 클러스터에 이상이 없다면, CSI 측 이슈를 디버깅 하기 위해 다음 섹션을 참고합니다.</p>
<h2 id="provisioning-volumes"><a class="header" href="#provisioning-volumes">Provisioning Volumes</a></h2>
<p>이슈는 Ceph-CSI 나 Ceph-CSI 에서 사용되는 사이드카 컨테이너에서 발생합니다.</p>
<p>Ceph-CSI 는 provisioner pod 에 여러 사이드카 컨테이너를 포함합니다: <code>csi-attacher</code>, <code>csi-resizer</code>, <code>csi-provisioner</code>, <code>csi-cephfsplugin</code>, <code>csi-snapshotter</code>, <code>liveness-promethues</code>.</p>
<p>CephFS 의 핵심 컨테이너는 <code>csi-cephfsplugin</code> 이고, RBD 프로비저너는 <code>csi-rbdplugin</code> 입니다.</p>
<p>사이드카 컨테이너들에 대한 설명입니다.</p>
<h3 id="csi-provisioner"><a class="header" href="#csi-provisioner">csi-provisioner</a></h3>
<p>external-provisioner 는 CSI 드라이버의 <code>ControllerCreateVolume()</code> 과 <code>ControllerDeleteVolume()</code> 함수를 호출함으로서 동적으로 볼륨을 프로비저닝하는 사이드카 컨테이너입니다. </p>
<p>만약 PVC 를 생성, 삭제하는 데 이슈가 있다면 <code>csi-provisioner</code> 사이드카 컨테이너의 로그를 확인하세요.</p>
<pre><code class="language-bash">kubectl -n rook-ceph logs deploy/csi-rbdplugin-provisioner -c csi-provisioner
</code></pre>
<h3 id="csi-resizer"><a class="header" href="#csi-resizer">csi-resizer</a></h3>
<p>CSI <code>external-resizer</code> 는 사용자가 PersistentVolumeClaim 에 더 큰 용량을 요청할 때의 업데이트 트리거를 받아 <code>ControllerExpandVolume</code> 을 호출합니다. </p>
<p>PVC 용량 확장에 이슈가 있다면 <code>csi-resizer</code> 사이드카의 로그를 확인합니다.</p>
<pre><code class="language-bash">kubectl -n rook-ceph logs deploy/csi-rbdplugin-provisioner -c csi-resizer
</code></pre>
<h3 id="csi-snapshotter"><a class="header" href="#csi-snapshotter">csi-snapshotter</a></h3>
<p>CSI external-snapshotter 사이드카는 <code>VolumeSnapshotContent</code> 리소스의 생성/삭제/업데이트 이벤트를 watch 하고, ceph-csi 컨테이너에게 스냅샷을 생성하거나 삭제하도록 명령합니다. external-snapshotter 에 대한 자세한 설명은 <a href="https://github.com/kubernetes-csi/external-snapshotter">이 곳</a> 을 참고합니다.</p>
<p><strong>쿠버네티스 1.17 부터 볼륨 스냅샷 피처는 beta 로 승격되었습니다. 1.20 부터, 기본으로 활성화 되어 있으며 비활성화 할 수 없습니다.</strong></p>
<p>올바른 snapshotter CRD 버전을 설치했는지 확인하세요. 아직 snapshotter controller 를 설치하지 않았다면, <a href="ceph_tools//ceph_storage/snapshots.html">Snapshots 가이드</a> 를 참고합니다.</p>
<pre><code class="language-bash">kubectl get crd | grep snapshot
</code></pre>
<pre><code class="language-bash">volumesnapshotclasses.snapshot.storage.k8s.io    2021-01-25T11:19:38Z
volumesnapshotcontents.snapshot.storage.k8s.io   2021-01-25T11:19:39Z
volumesnapshots.snapshot.storage.k8s.io          2021-01-25T11:19:40Z
</code></pre>
<p>위의 CRD 들은 <code>snapshotclass.yaml</code> 이나 <code>snapshot.yaml</code> 과 동일한 버전이어야 합니다. 그렇지 않다면, <code>VolumeSnapshot</code> 과 <code>VolumesnapshotContent</code> 리소스는 만들어지지 않습니다.</p>
<p>snapshot controller 는 <code>VolumeSnapshot</code> 과 <code>VolumeSnapshotContent</code> 리소스들을 만드는 데 책임이 있습니다. 만약 이 리소스들이 만들어지지 않는다면, snapshot-controller 컨테이너의 로그를 확인해야 합니다.</p>
<p>Rook 은 snapshotter 사이드카 컨테이너만 배포하며, controller 는 배포하지 않습니다. controller 는 CSI 드라이버 종류와 상관 없이 동작하기 때문에, 쿠버네티스 클러스터에 배포해 두는 것을 권장합니다.</p>
<p>만약 쿠버네티스 클러스터에 snapshot controller 가 설치되지 않았다면, 수동으로 설치가 가능합니다.</p>
<p>snapshot 을 생성/삭제 하는데 이슈가 있다면 csi-snapshotter 사이드카 컨테이너의 로그를 확인합니다.</p>
<pre><code class="language-bash">kubectl -n rook-ceph logs deploy/csi-rbdplugin-provisioner -c csi-snapshotter
</code></pre>
<p>아래와 같은 로그를 확인할 수 있을 것입니다.</p>
<pre><code class="language-bash">GRPC error: rpc error: code = Aborted desc = an operation with the given Volume ID
0001-0009-rook-ceph-0000000000000001-8d0ba728-0e17-11eb-a680-ce6eecc894de already &gt;exists.
</code></pre>
<p>보통 Ceph 클러스터의 자체 이슈나 통신 이슈일 수 있습니다. PVC 프로비저닝 이슈라면, 프로비저너를 재시작 하는 것이 도움이 될 수 있습니다. PVC 를 마운팅하는 데 이슈가 있다면, <code>csi-rbdplugin-xxxx</code> pod 을 재시작 (RBD) 하거나 CephFS 이슈에서는 <code>csi-cephfsplugin-xxxx</code> pod 을 재시작합니다.</p>
<h2 id="mounting-the-volume-to-application-pods"><a class="header" href="#mounting-the-volume-to-application-pods">Mounting the volume to application pods</a></h2>
<p>유저가 PVC 를 사용해 어플리케이션 Pod 을 생성하려고 한다면, 세 단계를 거치게 됩니다.</p>
<ul>
<li>CSI 드라이버 등록</li>
<li>volume attchment 리소스 생성</li>
<li>볼륨 stage, publish</li>
</ul>
<h3 id="csi-driver-registration"><a class="header" href="#csi-driver-registration">csi-driver registration</a></h3>
<p><code>csi-cephfsplugin-xxxx</code> 이나 <code>csi-rbdplugin-xxxx</code> pod 은 어플리케이션이 스케줄링 되는 모든 노드에서 동작하는 daemonset pod 입니다. 만약 어플리케이션이 스케줄링 되는 노드에 플러그인 pod 이 없다면 이슈가 생길 수 있습니다. 항상 플러그인 pod 이 동작 중인지 확인해야 합니다.</p>
<p>각 플러그인 pod 은 두 가지 중요한 컨테이너를 포함하고 있습니다. <code>driver-registrar</code> 와 <code>csi-rbdplugin</code> / <code>csi-cephfsplugin</code> 컨테이너 입니다. <code>liveness-prometheus</code> 컨테이너도 포함될 수 있습니다.</p>
<h3 id="driver-registrar"><a class="header" href="#driver-registrar">driver-registrar</a></h3>
<p>node-driver-registrar 는 Kubelet 에 CSI 드라이버를 등록하는 사이드카 컨테이너입니다. 더 많은 정보는 <a href="https://github.com/kubernetes-csi/node-driver-registrar">이 곳</a>을 참고합니다. </p>
<p>PVC 을 어플리케이션 Pod 에 attach 하는 데 이슈가 있다면 해당 어플리케이션 Pod 이 동작중인 노드의 driver-registrar 사이드카 컨테이너의 로그를 확인합니다.</p>
<pre><code class="language-bash">kubectl -n rook-ceph logs deploy/csi-rbdplugin -c driver-registrar
</code></pre>
<pre><code class="language-bash">I0120 12:28:34.231761  124018 main.go:112] Version: v2.0.1
I0120 12:28:34.233910  124018 connection.go:151] Connecting to unix:///csi/csi.sock
I0120 12:28:35.242469  124018 node_register.go:55] Starting Registration Server at: /registration/rook-ceph.rbd.csi.ceph.com-reg.sock
I0120 12:28:35.243364  124018 node_register.go:64] Registration Server started at: /registration/rook-ceph.rbd.csi.ceph.com-reg.sock
I0120 12:28:35.243673  124018 node_register.go:86] Skipping healthz server because port set to: 0
I0120 12:28:36.318482  124018 main.go:79] Received GetInfo call: &amp;InfoRequest{}
I0120 12:28:37.455211  124018 main.go:89] Received NotifyRegistrationStatus call: &amp;RegistrationStatus{PluginRegistered:true,Error:,}
E0121 05:19:28.658390  124018 connection.go:129] Lost connection to unix:///csi/csi.sock.
E0125 07:11:42.926133  124018 connection.go:129] Lost connection to unix:///csi/csi.sock.
</code></pre>
<p>로그에서 <code>RegistrationStatus{PluginRegistered:true,Error:,}</code> 를 확인했다면, kubelet 에 플러그인이 성공적으로 등록된 것입니다.</p>
<p>driver not found 에러가 확인된다면, <code>csi-xxxxplugin-xxx</code> pod 을 재시작 하는 것이 도움될 수 있습니다.</p>
<h2 id="volume-attachment"><a class="header" href="#volume-attachment">Volume Attachment</a></h2>
<p>각 프로비저너 pod 은 <code>csi-attacher</code> 라는 사이드카 컨테이너를 포함합니다.</p>
<h3 id="csi-attacher"><a class="header" href="#csi-attacher">csi-attacher</a></h3>
<p>external-attacher 는 CSI 드라이버에 <code>ControllerPublish</code> 와 <code>ControllerUnpublish</code> 함수를 호출함으로서 노드에 볼륨을 attach 합니다. 쿠버네티스 controller-manager 의 Attach/Detach 컨트롤러는 CSI 드라이버와 직접적인 인터페이스가 없기 때문에 이 external-attacher 는 필수적인 컴포넌트입니다. <a href="https://github.com/kubernetes-csi/external-attacher">이 곳</a> 에서 더 많은 정보를 확인하세요.</p>
<p>PVC 를 어플리케이션 pod 에 attach 하는 데 이슈가 있다면, 생성된 volumeattachment 리소스를 확인하고, provisioner pod 에서 csi-attacher 사이드카 컨테이너의 로그를 확인합니다.</p>
<pre><code class="language-bash">kubectl get volumeattachment
</code></pre>
<pre><code class="language-bash">NAME                                                                   ATTACHER                        PV                                         NODE       ATTACHED   AGE
csi-75903d8a902744853900d188f12137ea1cafb6c6f922ebc1c116fd58e950fc92   rook-ceph.cephfs.csi.ceph.com   pvc-5c547d2a-fdb8-4cb2-b7fe-e0f30b88d454   minikube   true       4m26s
</code></pre>
<pre><code class="language-bash">kubectl logs po/csi-rbdplugin-provisioner-d857bfb5f-ddctl -c csi-attacher
</code></pre>
<h2 id="cephfs-stale-operations"><a class="header" href="#cephfs-stale-operations">CephFS Stale operations</a></h2>
<p>어플리케이션 pod 이 스케줄링 된 노드의 <code>csi-cephfsplugin-xxxx</code> pod 에 stale (오래된) 마운트 커맨드가 있는지 확인합니다. </p>
<p>stale 마운트를 확인하기 위해서는 <code>csi-cephfsplugin-xxxx</code> pod 에 exec 로 접근해야 합니다.</p>
<p><code>kubectl get po -o wide</code> 커맨드로 어플리케이션이 동작 하고 있는 노드를 확인하고, 그 노드에 동작 중인 <code>csi-cephfsplugin-xxxx</code> pod 에 명령을 수행합니다.</p>
<pre><code class="language-bash">kubectl exec -it csi-cephfsplugin-tfk2g -c csi-cephfsplugin -- sh
ps -ef |grep mount

root          67      60  0 11:55 pts/0    00:00:00 grep mount
</code></pre>
<pre><code class="language-bash">ps -ef |grep ceph

root           1       0  0 Jan20 ?        00:00:26 /usr/local/bin/cephcsi --nodeid=minikube --type=cephfs --endpoint=unix:///csi/csi.sock --v=0 --nodeserver=true --drivername=rook-ceph.cephfs.csi.ceph.com --pidlimit=-1 --metricsport=9091 --forcecephkernelclient=true --metricspath=/metrics --enablegrpcmetrics=true
root          69      60  0 11:55 pts/0    00:00:00 grep ceph
</code></pre>
<p>커맨드가 stuck 되어 있는 것을 확인했다면, 노드에서 <code>dmesg</code> 로그를 확인합니다. <code>csi-cephfsplugin</code> pod 을 재시작하면 해결될 수 있습니다.</p>
<p>stuck 메시지를 찾지 못했다면, 통신 상태, Ceph 클러스터 상태, slow ops 를 확인하세요.</p>
<h2 id="rbd-stale-operations"><a class="header" href="#rbd-stale-operations">RBD Stale operations</a></h2>
<p>어플리케이션 Pod 이 스케줄링 된 노드의 <code>csi-rbdplugin-xxxx</code> pod 의 <code>map/mkfs/mount</code> 커맨드 stale 상태를 확인합니다.</p>
<p><code>csi-rbdplugin-xxxx</code> pod 에 exec 로 접근하여 stale 오퍼레이션 (<code>rbd map</code>, <code>rbd unmap</code>, <code>mkfs</code>, <code>mount</code>, <code>umount</code>) 을 확인해야 합니다.</p>
<p>마찬가지로 어플리케이션이 동작하고 있는 노드의 <code>csi-rbdplugin-xxxx</code> pod 에 접근합니다.</p>
<pre><code class="language-bash">kubectl exec -it csi-rbdplugin-vh8d5 -c csi-rbdplugin -- sh
</code></pre>
<pre><code class="language-bash">ps -ef |grep map

root     1297024 1296907  0 12:00 pts/0    00:00:00 grep map
</code></pre>
<pre><code class="language-bash">ps -ef |grep mount

root        1824       1  0 Jan19 ?        00:00:00 /usr/sbin/rpc.mountd
ceph     1041020 1040955  1 07:11 ?        00:03:43 ceph-mgr --fsid=ba41ac93-3b55-4f32-9e06-d3d8c6ff7334 --keyring=/etc/ceph/keyring-store/keyring --log-to-stderr=true --err-to-stderr=true --mon-cluster-log-to-stderr=true --log-stderr-prefix=debug  --default-log-to-file=false --default-mon-cluster-log-to-file=false --mon-host=[v2:10.111.136.166:3300,v1:10.111.136.166:6789] --mon-initial-members=a --id=a --setuser=ceph --setgroup=ceph --client-mount-uid=0 --client-mount-gid=0 --foreground --public-addr=172.17.0.6
root     1297115 1296907  0 12:00 pts/0    00:00:00 grep mount
</code></pre>
<pre><code class="language-bash">ps -ef |grep mkfs

root     1297291 1296907  0 12:00 pts/0    00:00:00 grep mkfs
</code></pre>
<pre><code class="language-bash">ps -ef |grep umount

root     1298500 1296907  0 12:01 pts/0    00:00:00 grep umount
</code></pre>
<pre><code class="language-bash">ps -ef |grep unmap

root     1298578 1296907  0 12:01 pts/0    00:00:00 grep unmap
</code></pre>
<p>커맨드가 stuck 되어 있는 것을 확인했다면, 노드의 <code>dmesg</code> 로그를 확인합니다. <code>csi-rbdplugin</code> pod 을 재시작하면 해결 될 수 있습니다.</p>
<p>stuck 메시지를 확인하지 못했다면, 통신 상태, Ceph 클러스터 상태, slow ops 를 확인합니다.</p>
<h2 id="dmesg-logs"><a class="header" href="#dmesg-logs">dmesg logs</a></h2>
<p><code>csi-rbdplugin-xxxx</code> pod 의 <code>csi-rbdplugin</code> 컨테이너가 동작 중인 PVC 마운트가 실패하는 노드에 대해 dmesg 로그를 확인합니다.</p>
<pre><code class="language-bash">dmesg
</code></pre>
<h2 id="rbd-commands"><a class="header" href="#rbd-commands">RBD Commands</a></h2>
<p>위 방법으로 해결이 되지 않는다면, ceph-csi pod 로그로부터 마지막 실행된 커맨드를 provisioner 나 플러그인 pod 에서 수동으로 실행시키고, 에러가 있는지 확인합니다. </p>
<pre><code class="language-bash">$ rbd ls --id=csi-rbd-node -m=10.111.136.166:6789 --key=AQDpIQhg+v83EhAAgLboWIbl+FL/nThJzoI3Fg==
</code></pre>
<p><code>-m</code> 옵션은 mon 엔드포인트이고, <code>--key</code> 옵션은 CSI 드라이버가 Ceph 클러스터에 접근하기 위한 키 입니다.</p>
<h2 id="node-loss"><a class="header" href="#node-loss">Node Loss</a></h2>
<p>노드가 죽으면, 해당 노드의 어플리케이션 pod 이 <code>Terminating</code> 상태로 남고 리스케줄링된 pod 은 <code>ContainerCreating</code> 상태가 됩니다.</p>
<p>이 pod 이 다른 노드에 정상적으로 동작하게 하기 위해서, <code>Terminating</code> 상태의 pod 을 강제 삭제합니다.</p>
<h3 id="force-deleting-the-pod"><a class="header" href="#force-deleting-the-pod">Force deleting the pod</a></h3>
<p><code>Terminating</code> 상태로 stuck 된 pod 을 강제 삭제합니다:</p>
<pre><code class="language-bash">$ kubectl -n rook-ceph delete pod my-app-69cd495f9b-nl6hf --grace-period 0 --force
</code></pre>
<p>강제 삭제 이후, 8-10 분정도 대기합니다. pod 이 running 상태가 되지 않으면, 노드를 blacklist 에 추가하기 위해 다음 섹션을 참고합니다.</p>
<h3 id="blocklisting-a-node"><a class="header" href="#blocklisting-a-node">Blocklisting a node</a></h3>
<p>타임아웃을 짧게 하고 failover 를 빠르게 하기 위해, 노드를 &quot;blocklisted&quot; 로 등록할 수 있습니다. </p>
<p>Ceph 버전이 Pacific(v16.2.x) 이상이라면, 아래 커맨드를 실행합니다.</p>
<pre><code class="language-bash">$ ceph osd blocklist add &lt;NODE_IP&gt; # blocklist 등록할 노드의 IP 를 조회해야 함
blocklisting &lt;NODE_IP&gt;
</code></pre>
<p>Ceph 버전이 Octopus(v15.2.x) 이하라면, 아래 커맨드를 실행합니다.</p>
<pre><code class="language-bash">$ ceph osd blacklist add &lt;NODE_IP&gt; # blocklist 등록할 노드의 IP 를 조회해야 함
blacklisting &lt;NODE_IP&gt;
</code></pre>
<p>위 커맨드를 실행한 후 pod 이 running 이 될 때까지 몇 분 대기합니다.</p>
<h3 id="removing-a-node-blocklist"><a class="header" href="#removing-a-node-blocklist">Removing a node blocklist</a></h3>
<p>노드가 완전히 오프라인 상태이고 더이상 blocklist 될 필요가 없다면, 노드를 blocklist 로부터 삭제합니다.</p>
<p>Ceph 버전이 Pacific(v16.2.x) 이상이라면, 아래 커맨드를 실행합니다.</p>
<pre><code class="language-bash">$ ceph osd blocklist rm &lt;NODE_IP&gt;
un-blocklisting &lt;NODE_IP&gt;
</code></pre>
<p>Ceph 버전이 Octopus(v15.2.x) 이하라면, 아래 커맨드를 실행합니다.</p>
<pre><code class="language-bash">$ ceph osd blacklist rm &lt;NODE_IP&gt; # get the node IP you want to blacklist
un-blacklisting &lt;NODE_IP&gt;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="monitor-health"><a class="header" href="#monitor-health">Monitor Health</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="osd-management"><a class="header" href="#osd-management">OSD Management</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="direct-tools"><a class="header" href="#direct-tools">Direct Tools</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-configuration"><a class="header" href="#advanced-configuration">Advanced Configuration</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-common-issues"><a class="header" href="#openshift-common-issues">Openshift Common Issues</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="disaster-recovery"><a class="header" href="#disaster-recovery">Disaster Recovery</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
